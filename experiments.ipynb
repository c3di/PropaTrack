{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24eb56a7c47fcd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "### Notebook to run experiments on the data supplied by the chair of functional materials at Saarland University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70237172f5fc33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 1\n",
    "### Run RAFT optical flow model on the data\n",
    "<font size=\"5\"> \n",
    "In this experiment we will run the RAFT optical flow model on the data to see how well it performs compared to classical optical flow methods.\n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "RAFT tracks the reaction front quite well. The reaction front is visible in the optical flow images but there is also a lot of noise.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c345de85fb8145",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from raft.utils import get_video_frames, plot_frames, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe78f1d47b1f36",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_path = \"videos/double_c_shape.mp4\"\n",
    "frames = get_video_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f8cbec9819aa6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the numbers in frames to see different images.\n",
    "img_batch = torch.stack([frames[50], frames[51]])\n",
    "plot_frames(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9a9e2fcb866d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.models.optical_flow.raft import Raft_Large_Weights\n",
    "\n",
    "model = raft_large(weights=Raft_Large_Weights.C_T_V2)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cb202f1496af2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img1_batch: torch.Tensor = preprocess(torch.stack([frames[130], frames[220]]))\n",
    "img2_batch: torch.Tensor = preprocess(torch.stack([frames[131], frames[221]]))\n",
    "\n",
    "plot_frames(img1_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d9556dadca2ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img1_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4e2c8fc9adcf3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_flows: torch.Tensor = model(img1_batch, img2_batch)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69928f67bbbff1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import flow_to_image\n",
    "flow_images = flow_to_image(predicted_flows)\n",
    "\n",
    "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
    "img1_batch = (img1_batch + 1.) / 2.\n",
    "\n",
    "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_images)]\n",
    "plot_frames(grid) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8ce6a",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "### Obtain reaction front from difference between frames\n",
    "<font size=\"5\"> \n",
    "In this experiment we will see how well we can obtain the reaction front by simply taking the difference between frames. We set pixels with very small and large differences between two frames to zero, as they seem to be noise. We also binarize the image to make the front more visible. For further denoising we apply a morphological operation called <i>Opening</i>, which removes all white spots smaller than a certain size. Mathematical morphology in the context of image processing deals with analysing and modifying structures in an image based on their geometrical properties. \n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "The reaction front is actually quite visible in the difference between frames. This is a very simple method to obtain the reaction front.  The method also seems to outperform the old optical flow method in terms of noise and also speed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cabb1949635f5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9de882a17a2f97",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"): \n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    frames = get_video_frames(video_path)\n",
    "    dir_name = \"results/experiment2/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    video_writer = cv2.VideoWriter(dir_name + video_name, fourcc, 10.0, (width, height))\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i+1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "        \n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "        \n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(flow)\n",
    "        \n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e62ffc9c4bbb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 3\n",
    "### Track the reaction fronts through the frames\n",
    "<font size=\"5\"> \n",
    "In this experiment we will try to track the reaction front and its offsprings throughout the frames. Since the images will be binarized we can use local connectivity of the pixels as a criterion for tracking. \n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "Segmenting the reaction front and it's offspring works quite good on a per frame basis, but it's difficult to track the different fronts throughout the frames.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551152f2f46d43b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology, measure\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388951942084d30",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "cmap = plt.get_cmap('tab20')\n",
    "props_old = None\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"): \n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    frames = get_video_frames(video_path)\n",
    "    video_writer = cv2.VideoWriter(\"results/experiment3/\" + video_name, fourcc, 10.0, (width, height))\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i+1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "        \n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "        \n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "        \n",
    "        # Label the connected components.\n",
    "        labeled_flow, num_labels = measure.label(flow, return_num=True)\n",
    "        labeled_flow = cv2.cvtColor(labeled_flow.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        for j in range(1, num_labels):\n",
    "            color = cmap(j)\n",
    "            color_array = (np.array(color[:-1]) * 255).astype(np.uint8)\n",
    "            flow = np.where(labeled_flow == j, color_array, flow)\n",
    "            \n",
    "        video_writer.write(flow)\n",
    "    \n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad77b56f4d1076",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 4\n",
    "### Apply optical flow to the binarized video frames\n",
    "<font size=\"5\"> \n",
    "In this experiment we will apply an optical flow algorithm to the binarized videos to get a vector field describing the speed of the reaction front at each point in time.\n",
    "\n",
    "04.05.2024:\n",
    "- Try adding skeletonization before calculating the optical flow and use opencv's calcOpticalFlowFarneback function.\n",
    "\n",
    "05.05.2024:\n",
    "- I tried playing around with the parameters of the algorithm like *poly_sigma* and *poly_n*.\n",
    "</font>\n",
    "<br><br>\n",
    "\n",
    "### Hypothesis\n",
    "<font size=\"5\">\n",
    "Since the frames are almost noise free I expect the optical flow algorithm to perform quite well on the data.\n",
    "</font>\n",
    "<br><br>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "The algorithm fails spectacularily, seemingly tracking the reaction fronts but also causing seemingly random splashes of greytones.\n",
    "\n",
    "04.05.2024:\n",
    "- After applying skeletonization there are no splashes anymore, but there still is noise and the flow seems to be too thick.\n",
    "\n",
    "05.05.2024:\n",
    "- Modifying the parameters doesn't improve the result.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4214c826a351e81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5720011bce3579c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "cmap = plt.get_cmap('plasma')\n",
    "props_old = None\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"): \n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    frames = get_video_frames(video_path)\n",
    "    \n",
    "    dir_name = \"results/experiment4_farneback_poly_sigma_0_2_poly_n_3/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "    video_writer = cv2.VideoWriter(dir_name + video_name, fourcc, 10.0, (width, height))\n",
    "    \n",
    "    for i, _ in tqdm(enumerate(frames[1:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        diff_flow0 = (frames[i+1] - frames[i]).astype(np.uint8)\n",
    "        diff_flow1 = (frames[i] - frames[i-1]).astype(np.uint8)\n",
    "        \n",
    "        # Threshold and binarize the image.\n",
    "        diff_flow0[diff_flow0 > 180] = 0\n",
    "        diff_flow0[diff_flow0 < 20] = 0\n",
    "        diff_flow0[diff_flow0 > 0] = 255\n",
    "        \n",
    "        diff_flow1[diff_flow1 > 180] = 0\n",
    "        diff_flow1[diff_flow1 < 20] = 0\n",
    "        diff_flow1[diff_flow1 > 0] = 255\n",
    "        \n",
    "        # Apply morphological opening.\n",
    "        diff_flow0 = morphology.opening(diff_flow0, footprint)\n",
    "        diff_flow1 = morphology.opening(diff_flow1, footprint)\n",
    "        \n",
    "        diff_flow0 = morphology.skeletonize(diff_flow0)\n",
    "        diff_flow1 = morphology.skeletonize(diff_flow1)\n",
    "        \n",
    "        diff_flow0 = np.where(diff_flow0 == 1, 255, 0).astype(np.uint8)\n",
    "        diff_flow1 = np.where(diff_flow1 == 1, 255, 0).astype(np.uint8)\n",
    "        \n",
    "        flow_field = cv2.calcOpticalFlowFarneback(diff_flow0, diff_flow1, None, 0.5, 3, 15, 3, 3, 0.2, 0)\n",
    "        \n",
    "        flow = np.sqrt(flow_field[:, :, 0]**2 + flow_field[:, :, 1]**2).astype(np.uint8)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR) \n",
    "                \n",
    "        video_writer.write(flow)\n",
    "        \n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107f6d6727dcd85",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Experiment 5\n",
    "### Apply skeletonization to the binarized video frames\n",
    "<font size=\"5\"> \n",
    "In this experiment we will apply skeletonization to the binarized videos to get a more sharp representation of the reaction front.\n",
    "</font>\n",
    "\n",
    "### Hypothesis\n",
    "<font size=\"5\">\n",
    "Will look like the previous experiments but with thin lines instead of blobs.\n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "Hypothesis turned out to be correct, there is still some noise present which I might be able to remove with more morphological operations.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cb4aaf8fd641e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0fb52b60bd329",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import setup_experiment\n",
    "from utils.frame_processing import front_from_frames\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8ceee842f2d95",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "footprint = morphology.disk(3)\n",
    "for video_name in os.listdir(\"videos/\"): \n",
    "    video_writer, frames = setup_experiment(video_name, \"results/experiment5_low_threshold_15/\")\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front = front_from_frames(frames[i], frames[i+1], footprint)  \n",
    "        front = cv2.cvtColor(front, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(front)\n",
    "        \n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d54236e984a9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 6\n",
    "### Plot evolution of reaction front on a single canvas\n",
    "<font size=\"5\"> \n",
    "In this experiment we will plot the reaction front at each time step on a single canvas to get an overview over its evolution in time from a single image.\n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "Fronts are clearly visible in most images. Only the videos with low resolutions and the one with the dots yield unclear images\n",
    "\n",
    "20.05.2024\n",
    "- It seems like v2 of the <i>front_from_frames</i> works better on images with small structures, while v1 works better on images with large structures.\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea53a4744913f28c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:08:29.318017Z",
     "start_time": "2024-06-06T09:08:27.399606Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b6ae4d69f61b9e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:09:28.372410Z",
     "start_time": "2024-06-06T09:08:30.796814Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 267.44it/s]\n",
      "198it [00:08, 24.69it/s]\n",
      "200it [00:08, 24.16it/s]\n",
      "230it [00:09, 23.04it/s]\n",
      "190it [00:07, 24.71it/s]\n",
      "160it [00:06, 24.49it/s]\n",
      "250it [00:01, 168.55it/s]\n",
      "211it [00:00, 265.55it/s]\n",
      "190it [00:08, 23.69it/s]\n"
     ]
    }
   ],
   "source": [
    "footprint = morphology.disk(3)\n",
    "for video_name in os.listdir(\"videos/\"): \n",
    "    frames = get_video_frames(\"videos/\" + video_name)\n",
    "    # Initialize the front\n",
    "    h, w = frames[0].shape\n",
    "    front =  np.zeros((h, w), dtype=np.uint8)\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front += front_from_frames(frames[i], frames[i+1], threshold_low=35)  \n",
    "    fig = plt.figure(dpi=200)\n",
    "    plt.imshow(front)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Reaction Front Progression - {}\".format(video_name))\n",
    "    result_dir = \"results/experiment6_v2_tl_35/\"\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    plt.savefig(result_dir + video_name + \".png\")       \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb400dd012591c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 7\n",
    "### Calculate distance travelled by reaction front\n",
    "<font size=\"5\"> \n",
    "In this experiment we will calculate the distance travelled by the reaction fronts between two frames by fitting contour lines to at each timestep and calculating their distance.\n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "Fronts are clearly visible in most images. Only the videos with low resolutions and the one with the dots yield unclear images\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55da7eaa3c49a32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed089ce7d29c4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/double_c_shape.mp4\")\n",
    "footprint = morphology.disk(3)\n",
    "# Initialize the front\n",
    "h, w = frames[0].shape\n",
    "front =  np.zeros((h, w), dtype=np.uint8)\n",
    "fronts = []\n",
    "for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "    front = front_from_frames(frames[i], frames[i+1], footprint)\n",
    "    fronts.append(front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32758716d4e70d55",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200)\n",
    "image = fronts[11]\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Contour Lines of Reaction Front\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c263b3a5cf13e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=\"5\">Calculate the average distance between the two fronts by determining for each pixel on front 1 the closest pixel on front 2 and then taking the average of these distances.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff090d9490f15b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "distances = cdist(np.squeeze(contours[0]), np.squeeze(contours[1]))\n",
    "print(f\"Average Distance: {np.mean(np.min(distances, axis=0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065cf8312f59f71",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=\"5\">Now we use scipy to fit splines to the contour lines and plot them.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47c09250fd44d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, indices_unique = np.unique(contours[0], axis=0, return_index=True)\n",
    "contour0 = contours[0][np.sort(indices_unique), :]\n",
    "\n",
    "X0 = np.squeeze(contour0)[::5, 0]\n",
    "Y0 = np.squeeze(contour0)[::5, 1]\n",
    "\n",
    "X1 = np.squeeze(contours[1])[::20, 0]\n",
    "Y1 = np.squeeze(contours[1])[::20, 1]\n",
    "\n",
    "tck0, u0 = interpolate.splprep([X0, Y0], s=1)\n",
    "tck1, u1 = interpolate.splprep([X1, Y1], s=1)\n",
    "\n",
    "out0 = np.array(interpolate.splev(u0, tck0)).T\n",
    "out1 = np.array(interpolate.splev(u1, tck1)).T\n",
    "\n",
    "tangent0 = interpolate.splev(u0, tck0, der=1)\n",
    "tangent1 = interpolate.splev(u1, tck1, der=1)\n",
    "\n",
    "normal0 = np.array([-tangent0[1], tangent0[0]]).T\n",
    "normal1 = np.array([-tangent1[1], tangent1[0]]).T\n",
    "\n",
    "normal0 = normal0 / np.linalg.norm(normal0, axis=1).reshape(-1, 1)\n",
    "normal1 = normal1 / np.linalg.norm(normal1, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa56a5fa85b0206",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(image)\n",
    "plt.plot(out0[:, 0], out0[:, 1], 'y', linewidth=0.25)\n",
    "plt.plot(out1[:, 0], out1[:, 1], 'g', linewidth=0.25)\n",
    "plt.quiver(out0[:, 0], out0[:, 1], normal0[:, 0], normal0[:, 1],\n",
    "           color='r', angles='xy', scale_units='xy', scale=0.05, width=0.001)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Splines fit to Contour Lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85585d9f53eb8532",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=\"5\">Calculate the average distance between the two fronts by determining for each pixel on front 1 the closest pixel on front 2 and then taking the average of these distances. Since the splines are sparser than the original reaction front it is to be expected that the average distance is slightly larger.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c169e8ea3489f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spline0 = np.array(out0).T\n",
    "spline1 = np.array(out1).T\n",
    "distances = cdist(spline0, spline1)\n",
    "print(f\"Average Distance: {np.mean(np.min(distances, axis=0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd40d8a1babcc32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 8\n",
    "### Fit splines at each timestep.\n",
    "<font size=\"5\"> \n",
    "In this experiment we will try to fit splines to the reaction front at every timestep and evaluate how well the splines fit the front, especially when it comes to fitting small offsprings of the\n",
    "front and noise.\n",
    "\n",
    "11.05.2024:\n",
    "- Sometimes the spline does not follow the contour by connecting each point to its neighbor, but rather jumps to another point father away on the contour. I calculated the distance of each point on the contour to its neighbour in the contour array and plotted a histogram of these distances. The histogram indicates that usually the jump occurs at the last point in the array.\n",
    "- I was able to get rid of many (but not all) of these points by just removing the last point of the contour array. This is more of a hack but it works for now.\n",
    "\n",
    "13.05.2024:\n",
    "- Using cv2.CHAIN_APPROX_NONE for the contours prevents situations where large jumps in the distances occur due to optimization of the contour.\n",
    "\n",
    "05.06.2023:\n",
    "- Use an updated scheme for handling outliers of the contour.\n",
    "</font>\n",
    "\n",
    "### Hypothesis\n",
    "<font size=\"5\"> \n",
    "The splines will fit the front quite well, but will have problems with small offsprings and noise.\n",
    "</font> \n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc058e06bdefd31",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T09:00:31.711978Z",
     "start_time": "2024-06-05T09:00:31.708680Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames, two_opt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from skimage import morphology\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18ec9d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-05T10:22:14.703935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:20,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "footprint = morphology.disk(3)\n",
    "cmap = plt.get_cmap('tab20')\n",
    "# Initialize the front\n",
    "h, w = frames[0].shape\n",
    "blank_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "    front = front_from_frames(frames[i], frames[i+1])\n",
    "    contours, _ = cv2.findContours(front, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    contours = [np.squeeze(contour) for contour in contours if cv2.arcLength(contour, False) > 25]\n",
    "    \n",
    "    width = max((len(contours) + 1) * 5, 15)\n",
    "    fig, ax = plt.subplots(dpi=200,\n",
    "                           figsize=(width, 5),\n",
    "                           nrows=1,\n",
    "                           ncols=len(contours) + 1,\n",
    "                           width_ratios=[0.5] * len(contours) + [1],\n",
    "                           squeeze=False)\n",
    "    ax[-1, len(contours)].imshow(blank_canvas)\n",
    "    ax[-1, len(contours)].set_title(\"Splines fit to Contour Lines\")\n",
    "    for j, contour in enumerate(contours):\n",
    "        # The contour wraps around the image, so we only take the first half.\n",
    "        _, indices_unique = np.unique(contour, axis=0, return_index=True)\n",
    "        contour = contour[np.sort(indices_unique), :]\n",
    "        \n",
    "        steps = max(int(len(contour) * 0.025), 1)\n",
    "\n",
    "        if (len(contour) - 1) % steps == 0:\n",
    "            contour = contour[::steps]\n",
    "        else:\n",
    "            # Make sure the last point is included, so contours don't get cut off.\n",
    "            contour = np.append(contour[::steps], contour[-1:], axis=0)\n",
    "            \n",
    "        intra_dists = np.diagonal(cdist(contour[:-1], contour[1:]))\n",
    "        mean_dist = np.mean(intra_dists)\n",
    "        \n",
    "        outlier_dists = np.where(intra_dists > 3 * mean_dist)\n",
    "        if len(outlier_dists[0]) >= 1:\n",
    "            if outlier_dists[0][0] >= len(intra_dists) - 3:\n",
    "                contour = contour[:-(len(intra_dists) - outlier_dists[0][0])]\n",
    "            else:\n",
    "                contour_truncated = contour[:outlier_dists[0][0] + 1]\n",
    "                contour_rest = contour[outlier_dists[0][-1] + 1:]\n",
    "                contour = np.concatenate((contour_rest[::-1], contour_truncated))\n",
    "    \n",
    "        ax[-1, j].bar(range(len(intra_dists)), intra_dists, color=cmap(j))\n",
    "        ax[-1, j].set_title(f\"Point distances contour {j}\")\n",
    "        ax[-1, j].axhline(y=3*mean_dist, color='r', linestyle='--')\n",
    "        X = contour[:, 0]\n",
    "        Y = contour[:, 1]\n",
    "        \n",
    "        k = 1 if len(contour) < 10 else 3\n",
    "        \n",
    "        tck, u = interpolate.splprep([X, Y], s=2, k=k)\n",
    "        out = np.array(interpolate.splev(u, tck)).T\n",
    "        ax[-1, len(contours)].plot(out[:, 0], out[:, 1], linewidth=0.25, color=cmap(j))\n",
    "        \n",
    "    experiment_dir = \"results/experiment8_vertical_lines_outlier_case_filered/\"\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    fig.suptitle(f\"Analysis of Frame {i}\")\n",
    "    plt.savefig(experiment_dir + f\"frame_{i}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb7af28645b448",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 9\n",
    "### Fit splines at each timestep.\n",
    "<font size=\"5\"> \n",
    "Based on the spline fitting procedure developed in experiment 8, create a vector field indicating the wavefront speed for the simplest video <i>no_shapes.mp4</i>.\n",
    "\n",
    "### Hypothesis\n",
    "<font size=\"5\"> \n",
    "The code will be specifically tailored to this video, so it should work quite well.\n",
    "</font> \n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "\n",
    "16.05.2024:\n",
    "- I did not find a good way to obtain the speed, but the directional field already looks decent. \n",
    "\n",
    "20.05.2024:\n",
    "- I updated the tresholding in <i>front_from_frames</i> and now the normals are more accurate.\n",
    "\n",
    "26.05.2024:\n",
    "- Optimizing contours using a version of the 2-opt algorithm for the TSP problem made the results worse by a alot.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34a7f25500baf4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T15:19:32.273416Z",
     "start_time": "2024-06-06T15:19:31.745670Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames, process_contour, contours_from_front\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate \n",
    "from scipy.spatial.distance import cdist\n",
    "from utils.visualization import dfki_cmap, dist_to_idx\n",
    "from utils.documentation import Documenter"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [01:12,  2.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\"video name\": \"vertical_lines.mp4\",\n",
    "          \"threshold_low\": 25,\n",
    "          \"min_length\": 5}\n",
    "\n",
    "experiment_dir = \"results\"\n",
    "experiment_name = \"experiment9_vertical_lines_better_colors\"\n",
    "\n",
    "Doc = Documenter(experiment_dir, experiment_name, params, True)\n",
    "Doc.comment(\"Purpose of this experiment is to see whether scaling the width argument\\n\"\n",
    "            \"of plt.quiver by dist_to_index(min_dist) makes the arrows fit to the gap\\n\"\n",
    "            \"between contours.\")\n",
    "\n",
    "frames = get_video_frames(f\"videos/{params['video name']}\")\n",
    "\n",
    "# Initialize the front\n",
    "h, w = frames[0].shape\n",
    "blank_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "plt.imshow(blank_canvas)\n",
    "fig = plt.figure(dpi=200)\n",
    "for i, _ in tqdm(enumerate(frames[:-2])):\n",
    "    front = front_from_frames(frames[i], frames[i+1], threshold_low=params['threshold_low'])\n",
    "    front_next = front_from_frames(frames[i+1], frames[i+2], threshold_low=params['threshold_low'])\n",
    "    \n",
    "    contours = contours_from_front(front, min_length=params['min_length'])\n",
    "    contours_next = contours_from_front(front_next, min_length=params['min_length'])\n",
    "    \n",
    "    for j, contour in enumerate(contours):\n",
    "        \n",
    "        contour = process_contour(contour)\n",
    "        \n",
    "        X = contour[:, 0]\n",
    "        Y = contour[:, 1]\n",
    "        \n",
    "        k = 1 if len(contour) < 10 else 3\n",
    "        \n",
    "        tck, u = interpolate.splprep([X, Y], s=2, k=k)\n",
    "        front_spline = np.array(interpolate.splev(u, tck)).T\n",
    "        \n",
    "        tangents = interpolate.splev(u, tck, der=1)\n",
    "        normals = np.array([-tangents[1], tangents[0]]).T  \n",
    "        normals = normals / np.linalg.norm(normals, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        plt.plot(front_spline[:, 0],  front_spline[:, 1], linewidth=0.25, color=\"green\")\n",
    "        \n",
    "        for spline_point, spline_normal in zip(front_spline, normals):\n",
    "            min_dist = w**2 + h**2 # Distances can't be larger than image diagonal\n",
    "            for contour_next in contours_next:\n",
    "                distances = cdist(np.expand_dims(spline_point, axis=0), contour_next)\n",
    "                dist = np.min(distances)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "            \n",
    "            if min_dist > 10:\n",
    "                Doc.log(f\"min_dist: {min_dist} in frame {i}\")\n",
    "        \n",
    "            plt.quiver(spline_point[0],\n",
    "                       spline_point[1],\n",
    "                       spline_normal[0],\n",
    "                       spline_normal[1],\n",
    "                       color=dfki_cmap(dist_to_idx(min_dist)),\n",
    "                       angles='xy',\n",
    "                       scale_units='xy',\n",
    "                       scale=1/(min_dist + 10e-6),\n",
    "                       width=0.001)\n",
    "            \n",
    "Doc.close()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Vector Field of Reaction Front for no_shape.mp4\")\n",
    "plt.savefig(experiment_dir + \"/\" + experiment_name + \"/vector_field.png\")\n",
    "plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T15:47:22.982401Z",
     "start_time": "2024-06-06T15:46:05.817920Z"
    }
   },
   "id": "63086f3caa56b83c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "44fb62dc5b80860c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=\"5\"> Visualize all steps of the current video processing pipeline for my presentation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9df704e87d02b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.visualization import plot_pipeline_steps\n",
    "plot_pipeline_steps(\"videos/no_shape.mp4\", 104, 105, img_dir=\"results/visualization/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6de3d7ebe4fbf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 10\n",
    "### Track tiny reaction fronts.\n",
    "<font size=\"5\"> \n",
    "In some videos such as <i>vertical_lines.mp4</i> some of the offsprings of the initial reaction front are very small and thus hard to track. The skeletonization als results in deformed front that deviate a lot from the smooth curved fronts that are observed in other parts of the video.\n",
    "\n",
    "21.05.2024:\n",
    "- I try out different types of image processing steps and denoising to improve the quality of small reaction fronts.\n",
    "\n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "\n",
    "20.05.2024:\n",
    "- When the front passes through small gaps a part of it's trail is recognized as belonging to the front itself, which gives the front a rounded appearance.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2e4ec61f319c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology, filters, restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb900b88b13c769",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "frame0 = frames[110]\n",
    "frame1 = frames[111]\n",
    "\n",
    "frame0[frame0 > 35] = 255\n",
    "frame0[frame0 < 35] = 0\n",
    "frame1[frame1 > 35] = 255\n",
    "frame1[frame1 < 35] = 0\n",
    "\n",
    "front = morphology.opening(frame1-frame0, morphology.disk(3))\n",
    "plt.imshow(frames[112], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b2d49ff3d67d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=\"5\">Check out the effects of Sobel filtering on the front.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf5ff911405c52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "edges = filters.sobel(frames[110])\n",
    "\n",
    "low = 0.1\n",
    "high = 0.15\n",
    "\n",
    "lowt = (edges > low).astype(int)\n",
    "hight = (edges > high).astype(int)\n",
    "\n",
    "plt.imshow(edges, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6cc07a74245d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visualize areas of low contrast via local entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b594b27b5c698e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "entr_img = filters.rank.entropy(frames[110], morphology.disk(10))\n",
    "\n",
    "plt.imshow(entr_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c88569cf850fa1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "denoised0 = restoration.denoise_tv_chambolle(frames[110], weight=0.5)\n",
    "denoised1 = restoration.denoise_tv_chambolle(frames[111], weight=0.5)\n",
    "\n",
    "plt.imshow(denoised1 - denoised0, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8960d1a4d454910",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
