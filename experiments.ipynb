{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24eb56a7c47fcd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "### Notebook to run experiments on the data supplied by the chair of functional materials at Saarland University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70237172f5fc33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 1\n",
    "### Run RAFT optical flow model on the data\n",
    "<font size=\"5\"> \n",
    "In this experiment we will run the RAFT optical flow model on the data to see how well it performs compared to classical optical flow methods.\n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "RAFT tracks the reaction front quite well. The reaction front is visible in the optical flow images but there is also a lot of noise.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c345de85fb8145",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T09:04:18.548120Z",
     "start_time": "2024-04-28T09:04:11.849006Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from raft.utils import get_video_frames, plot_frames, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafe78f1d47b1f36",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:34:25.761698Z",
     "start_time": "2024-04-27T13:34:23.644046Z"
    }
   },
   "outputs": [],
   "source": [
    "video_path = \"videos/DoubleC-shape/double_c_shape.mp4\"\n",
    "frames = get_video_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Change the numbers in frames to see different images.\n",
    "img_batch = torch.stack([frames[50], frames[51]])\n",
    "plot_frames(img_batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "249f8cbec9819aa6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b9a9e2fcb866d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:34:26.243530Z",
     "start_time": "2024-04-27T13:34:26.134950Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.models.optical_flow.raft import Raft_Large_Weights\n",
    "\n",
    "model = raft_large(weights=Raft_Large_Weights.C_T_V2)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "img1_batch: torch.Tensor = preprocess(torch.stack([frames[130], frames[220]]))\n",
    "img2_batch: torch.Tensor = preprocess(torch.stack([frames[131], frames[221]]))\n",
    "\n",
    "plot_frames(img1_batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd1cb202f1496af2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d4e2c8fc9adcf3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:34:37.216463Z",
     "start_time": "2024-04-27T13:34:26.534545Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_flows: torch.Tensor = model(img1_batch, img2_batch)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchvision.utils import flow_to_image\n",
    "flow_images = flow_to_image(predicted_flows)\n",
    "\n",
    "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
    "img1_batch = (img1_batch + 1.) / 2.\n",
    "\n",
    "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_images)]\n",
    "plot_frames(grid) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d69928f67bbbff1f"
  },
  {
   "cell_type": "markdown",
   "id": "fdc8ce6a",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "### Obtain reaction front from difference between frames\n",
    "<font size=\"5\"> \n",
    "In this experiment we will see how well we can obtain the reaction front by simply taking the difference between frames. We set pixels with very small and large differences between two frames to zero, as they seem to be noise. We also binarize the image to make the front more visible. For further denoising we apply a morphological operation called <i>Opening</i>, which removes all white spots smaller than a certain size. Mathematical morphology in the context of image processing deals with analysing and modifying structures in an image based on their geometrical properties. \n",
    "</font>\n",
    "\n",
    "### Results\n",
    "<font size=\"5\"> \n",
    "The reaction front is actually quite visible in the difference between frames. This is a very simple method to obtain the reaction front.  The method also seems to outperform the old optical flow method in terms of noise and also speed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T07:22:00.793053Z",
     "start_time": "2024-04-29T07:21:57.966339Z"
    }
   },
   "id": "fe7cabb1949635f5",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9de882a17a2f97",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T07:26:12.963846Z",
     "start_time": "2024-04-29T07:24:54.423501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flow for comb_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [00:12, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flow for c_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:12, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flow for double_c_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [00:15, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flow for lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190it [00:12, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flow for no_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [00:10, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flow for vertical_lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190it [00:12, 15.82it/s]\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"): \n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    frames = get_video_frames(video_path)\n",
    "    video_writer = cv2.VideoWriter(\"results/experiment2/\" + video_name, fourcc, 10.0, (width, height))\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i+1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "        \n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "        \n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(flow)\n",
    "        \n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e9e0f54e5421f8e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
