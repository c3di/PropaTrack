{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24eb56a7c47fcd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "### Notebook to run experiments on the data supplied by the chair of functional materials at Saarland University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70237172f5fc33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 1\n",
    "### Run RAFT optical flow model on the data\n",
    "In this experiment we will run the RAFT optical flow model on the data to see how well it performs compared to classical optical flow methods.\n",
    "\n",
    "\n",
    "### Results\n",
    "RAFT tracks the reaction front quite well. The reaction front is visible in the optical flow images but there is also a lot of noise."
   ]
  },
  {
   "cell_type": "code",
   "id": "74c345de85fb8145",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import torch\n",
    "from src.raft.utils import get_video_frames, plot_frames, preprocess"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cafe78f1d47b1f36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "video_path = \"videos/double_c_shape.mp4\"\n",
    "frames = get_video_frames(video_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "249f8cbec9819aa6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Change the numbers in frames to see different images.\n",
    "img_batch = torch.stack([frames[50], frames[51]])\n",
    "plot_frames(img_batch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10b9a9e2fcb866d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.models.optical_flow.raft import Raft_Large_Weights\n",
    "\n",
    "model = raft_large(weights=Raft_Large_Weights.C_T_V2)\n",
    "model = model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd1cb202f1496af2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "img1_batch: torch.Tensor = preprocess(torch.stack([frames[130], frames[220]]))\n",
    "img2_batch: torch.Tensor = preprocess(torch.stack([frames[131], frames[221]]))\n",
    "\n",
    "plot_frames(img1_batch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c96d9556dadca2ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "img1_batch.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5d4e2c8fc9adcf3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predicted_flows: torch.Tensor = model(img1_batch, img2_batch)[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d69928f67bbbff1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from torchvision.utils import flow_to_image\n",
    "\n",
    "flow_images = flow_to_image(predicted_flows)\n",
    "\n",
    "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
    "img1_batch = (img1_batch + 1.0) / 2.0\n",
    "\n",
    "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_images)]\n",
    "plot_frames(grid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdc8ce6a",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "### Obtain reaction front from difference between frames\n",
    "In this experiment we will see how well we can obtain the reaction front by simply taking the difference between frames. We set pixels with very small and large differences between two frames to zero, as they seem to be noise. We also binarize the image to make the front more visible. For further denoising we apply a morphological operation called <i>Opening</i>, which removes all white spots smaller than a certain size. Mathematical morphology in the context of image processing deals with analysing and modifying structures in an image based on their geometrical properties. \n",
    "\n",
    "\n",
    "### Results\n",
    "The reaction front is actually quite visible in the difference between frames. This is a very simple method to obtain the reaction front.  The method also seems to outperform the old optical flow method in terms of noise and also speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe7cabb1949635f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from src.utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e9de882a17a2f97",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    frames = get_video_frames(video_path)\n",
    "    dir_name = \"results/experiment2/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    video_writer = cv2.VideoWriter(dir_name + video_name, fourcc, 10.0, (width, height))\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i + 1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "\n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "\n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(flow)\n",
    "\n",
    "    video_writer.release()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b8e62ffc9c4bbb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 3\n",
    "### Track the reaction fronts through the frames\n",
    "In this experiment we will try to track the reaction front and its offsprings throughout the frames. Since the images will be binarized we can use local connectivity of the pixels as a criterion for tracking. \n",
    "\n",
    "### Results\n",
    "Segmenting the reaction front and it's offspring works quite good on a per frame basis, but it's difficult to track the different fronts throughout the frames."
   ]
  },
  {
   "cell_type": "code",
   "id": "c551152f2f46d43b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from skimage import morphology, measure\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from src.utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1388951942084d30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "props_old = None\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    frames = get_video_frames(video_path)\n",
    "    video_writer = cv2.VideoWriter(\n",
    "        \"results/experiment3/\" + video_name, fourcc, 10.0, (width, height)\n",
    "    )\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i + 1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "\n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "\n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "\n",
    "        # Label the connected components.\n",
    "        labeled_flow, num_labels = measure.label(flow, return_num=True)\n",
    "        labeled_flow = cv2.cvtColor(labeled_flow.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        for j in range(1, num_labels):\n",
    "            color = cmap(j)\n",
    "            color_array = (np.array(color[:-1]) * 255).astype(np.uint8)\n",
    "            flow = np.where(labeled_flow == j, color_array, flow)\n",
    "\n",
    "        video_writer.write(flow)\n",
    "\n",
    "    video_writer.release()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aaad77b56f4d1076",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 4\n",
    "### Apply optical flow to the binarized video frames\n",
    "In this experiment we will apply an optical flow algorithm to the binarized videos to get a vector field describing the speed of the reaction front at each point in time.\n",
    "\n",
    "04.05.2024:\n",
    "- Try adding skeletonization before calculating the optical flow and use opencv's calcOpticalFlowFarneback function.\n",
    "\n",
    "05.05.2024:\n",
    "- I tried playing around with the parameters of the algorithm like *poly_sigma* and *poly_n*.\n",
    "<br><br>\n",
    "\n",
    "### Hypothesis\n",
    "Since the frames are almost noise free I expect the optical flow algorithm to perform quite well on the data.\n",
    "<br><br>\n",
    "\n",
    "### Results\n",
    "The algorithm fails spectacularly, seemingly tracking the reaction fronts but also causing seemingly random splashes of gray tones.\n",
    "\n",
    "04.05.2024:\n",
    "- After applying skeletonization there are no splashes anymore, but there still is noise and the flow seems to be too thick.\n",
    "\n",
    "05.05.2024:\n",
    "- Modifying the parameters doesn't improve the result."
   ]
  },
  {
   "cell_type": "code",
   "id": "b4214c826a351e81",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from skimage import morphology\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from src.utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5720011bce3579c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "cmap = plt.get_cmap(\"plasma\")\n",
    "props_old = None\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    frames = get_video_frames(video_path)\n",
    "\n",
    "    dir_name = \"results/experiment4_farneback_poly_sigma_0_2_poly_n_3/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    video_writer = cv2.VideoWriter(dir_name + video_name, fourcc, 10.0, (width, height))\n",
    "\n",
    "    for i, _ in tqdm(enumerate(frames[1:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        diff_flow0 = (frames[i + 1] - frames[i]).astype(np.uint8)\n",
    "        diff_flow1 = (frames[i] - frames[i - 1]).astype(np.uint8)\n",
    "\n",
    "        # Threshold and binarize the image.\n",
    "        diff_flow0[diff_flow0 > 180] = 0\n",
    "        diff_flow0[diff_flow0 < 20] = 0\n",
    "        diff_flow0[diff_flow0 > 0] = 255\n",
    "\n",
    "        diff_flow1[diff_flow1 > 180] = 0\n",
    "        diff_flow1[diff_flow1 < 20] = 0\n",
    "        diff_flow1[diff_flow1 > 0] = 255\n",
    "\n",
    "        # Apply morphological opening.\n",
    "        diff_flow0 = morphology.opening(diff_flow0, footprint)\n",
    "        diff_flow1 = morphology.opening(diff_flow1, footprint)\n",
    "\n",
    "        diff_flow0 = morphology.skeletonize(diff_flow0)\n",
    "        diff_flow1 = morphology.skeletonize(diff_flow1)\n",
    "\n",
    "        diff_flow0 = np.where(diff_flow0 == 1, 255, 0).astype(np.uint8)\n",
    "        diff_flow1 = np.where(diff_flow1 == 1, 255, 0).astype(np.uint8)\n",
    "\n",
    "        flow_field = cv2.calcOpticalFlowFarneback(\n",
    "            diff_flow0, diff_flow1, None, 0.5, 3, 15, 3, 3, 0.2, 0\n",
    "        )\n",
    "\n",
    "        flow = np.sqrt(flow_field[:, :, 0] ** 2 + flow_field[:, :, 1] ** 2).astype(np.uint8)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        video_writer.write(flow)\n",
    "\n",
    "    video_writer.release()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3107f6d6727dcd85",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Experiment 5\n",
    "### Apply skeletonization to the binarized video frames\n",
    "In this experiment we will apply skeletonization to the binarized videos to get a more sharp representation of the reaction front.\n",
    "\n",
    "### Hypothesis\n",
    "Will look like the previous experiments but with thin lines instead of blobs.\n",
    "\n",
    "### Results\n",
    "Hypothesis turned out to be correct, there is still some noise present which I might be able to remove with more morphological operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cb4aaf8fd641e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "id": "92a0fb52b60bd329",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from src.utils.video_handling import setup_experiment\n",
    "from src.utils.frame_processing import front_from_frames\n",
    "import cv2\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86e8ceee842f2d95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "footprint = morphology.disk(3)\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    video_writer, frames = setup_experiment(video_name, \"results/experiment5_low_threshold_15/\")\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front = front_from_frames(frames[i], frames[i + 1], footprint)\n",
    "        front = cv2.cvtColor(front, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(front)\n",
    "\n",
    "    video_writer.release()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "631d54236e984a9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 6\n",
    "### Plot evolution of reaction front on a single canvas\n",
    "In this experiment we will plot the reaction front at each time step on a single canvas to get an overview over its evolution in time from a single image.\n",
    "\n",
    "### Results \n",
    "Fronts are clearly visible in most images. Only the videos with low resolutions and the one with the dots yield unclear images\n",
    "\n",
    "20.05.2024\n",
    "- It seems like v2 of the <i>front_from_frames</i> works better on images with small structures, while v1 works better on images with large structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea53a4744913f28c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.frame_processing import front_from_frames\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b6ae4d69f61b9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "footprint = morphology.disk(3)\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    frames = get_video_frames(\"videos/\" + video_name)\n",
    "    # Initialize the front\n",
    "    h, w = frames[0].shape\n",
    "    front = np.zeros((h, w), dtype=np.uint8)\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front += front_from_frames(frames[i], frames[i + 1], threshold=35)\n",
    "    fig = plt.figure(dpi=200)\n",
    "    plt.imshow(front)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Reaction Front Progression - {}\".format(video_name))\n",
    "    result_dir = \"results/experiment6_v2_tl_35/\"\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    plt.savefig(result_dir + video_name + \".png\")\n",
    "    plt.close(fig)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9fb400dd012591c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 7\n",
    "### Calculate distance travelled by reaction front\n",
    "In this experiment we will calculate the distance travelled by the reaction fronts between two frames by fitting contour lines to at each timestep and calculating their distance.\n",
    "\n",
    "### Results\n",
    "Fronts are clearly visible in most images. Only the videos with low resolutions and the one with the dots yield unclear images"
   ]
  },
  {
   "cell_type": "code",
   "id": "a55da7eaa3c49a32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.frame_processing import front_from_frames\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import interpolate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8ed089ce7d29c4b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "frames = get_video_frames(\"videos/double_c_shape.mp4\")\n",
    "footprint = morphology.disk(3)\n",
    "# Initialize the front\n",
    "h, w = frames[0].shape\n",
    "front = np.zeros((h, w), dtype=np.uint8)\n",
    "fronts = []\n",
    "for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "    front = front_from_frames(frames[i], frames[i + 1], footprint)\n",
    "    fronts.append(front)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32758716d4e70d55",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig = plt.figure(dpi=200)\n",
    "image = fronts[11]\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Contour Lines of Reaction Front\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed9c263b3a5cf13e",
   "metadata": {
    "collapsed": false
   },
   "source": "Calculate the average distance between the two fronts by determining for each pixel on front 1 the closest pixel on front 2 and then taking the average of these distances."
  },
  {
   "cell_type": "code",
   "id": "eff090d9490f15b9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "distances = cdist(np.squeeze(contours[0]), np.squeeze(contours[1]))\n",
    "print(f\"Average Distance: {np.mean(np.min(distances, axis=0))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2065cf8312f59f71",
   "metadata": {
    "collapsed": false
   },
   "source": "Now we use scipy to fit splines to the contour lines and plot them."
  },
  {
   "cell_type": "code",
   "id": "1b47c09250fd44d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_, indices_unique = np.unique(contours[0], axis=0, return_index=True)\n",
    "contour0 = contours[0][np.sort(indices_unique), :]\n",
    "\n",
    "X0 = np.squeeze(contour0)[::5, 0]\n",
    "Y0 = np.squeeze(contour0)[::5, 1]\n",
    "\n",
    "X1 = np.squeeze(contours[1])[::20, 0]\n",
    "Y1 = np.squeeze(contours[1])[::20, 1]\n",
    "\n",
    "tck0, u0 = interpolate.splprep([X0, Y0], s=1)\n",
    "tck1, u1 = interpolate.splprep([X1, Y1], s=1)\n",
    "\n",
    "out0 = np.array(interpolate.splev(u0, tck0)).T\n",
    "out1 = np.array(interpolate.splev(u1, tck1)).T\n",
    "\n",
    "tangent0 = interpolate.splev(u0, tck0, der=1)\n",
    "tangent1 = interpolate.splev(u1, tck1, der=1)\n",
    "\n",
    "normal0 = np.array([-tangent0[1], tangent0[0]]).T\n",
    "normal1 = np.array([-tangent1[1], tangent1[0]]).T\n",
    "\n",
    "normal0 = normal0 / np.linalg.norm(normal0, axis=1).reshape(-1, 1)\n",
    "normal1 = normal1 / np.linalg.norm(normal1, axis=1).reshape(-1, 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fa56a5fa85b0206",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(image)\n",
    "plt.plot(out0[:, 0], out0[:, 1], \"y\", linewidth=0.25)\n",
    "plt.plot(out1[:, 0], out1[:, 1], \"g\", linewidth=0.25)\n",
    "plt.quiver(\n",
    "    out0[:, 0],\n",
    "    out0[:, 1],\n",
    "    normal0[:, 0],\n",
    "    normal0[:, 1],\n",
    "    color=\"r\",\n",
    "    angles=\"xy\",\n",
    "    scale_units=\"xy\",\n",
    "    scale=0.05,\n",
    "    width=0.001,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Splines fit to Contour Lines\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85585d9f53eb8532",
   "metadata": {
    "collapsed": false
   },
   "source": "Calculate the average distance between the two fronts by determining for each pixel on front 1 the closest pixel on front 2 and then taking the average of these distances. Since the splines are sparser than the original reaction front it is to be expected that the average distance is slightly larger."
  },
  {
   "cell_type": "code",
   "id": "7b3c169e8ea3489f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "spline0 = np.array(out0).T\n",
    "spline1 = np.array(out1).T\n",
    "distances = cdist(spline0, spline1)\n",
    "print(f\"Average Distance: {np.mean(np.min(distances, axis=0))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cd40d8a1babcc32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 8\n",
    "### Fit splines at each timestep.\n",
    "In this experiment we will try to fit splines to the reaction front at every timestep and evaluate how well the splines fit the front, especially when it comes to fitting small offsprings of the\n",
    "front and noise.\n",
    "\n",
    "11.05.2024:\n",
    "- Sometimes the spline does not follow the contour by connecting each point to its neighbor, but rather jumps to another point father away on the contour. I calculated the distance of each point on the contour to its neighbour in the contour array and plotted a histogram of these distances. The histogram indicates that usually the jump occurs at the last point in the array.\n",
    "- I was able to get rid of many (but not all) of these points by just removing the last point of the contour array. This is more of a hack, but it works for now.\n",
    "\n",
    "13.05.2024:\n",
    "- Using cv2.CHAIN_APPROX_NONE for the contours prevents situations where large jumps in the distances occur due to optimization of the contour.\n",
    "\n",
    "05.06.2024:\n",
    "- Use an updated scheme for handling outliers of the contour.\n",
    "\n",
    "18.06.2024:\n",
    "- Incorporate some of the new functions that abstract away functionality for processing contours.\n",
    "\n",
    "### Hypothesis\n",
    "The splines will fit the front quite well but will have problems with small offsprings and noise.\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "9bc058e06bdefd31",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.frame_processing import (\n",
    "    front_from_frames,\n",
    "    contours_from_front,\n",
    "    remove_duplicates,\n",
    "    sample_contour,\n",
    "    handle_outliers,\n",
    "    spline_from_contour,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "from src.utils.documentation import Documenter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f18ec9d",
   "metadata": {},
   "source": [
    "VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "\n",
    "for video_name in [\"c_shape.mp4\"]:\n",
    "\n",
    "    experiment_dir = \"results/experiment8\"\n",
    "    experiment_name = f\"{video_name.split('.')[0]}_jumps_in_contour\"\n",
    "    # Doc = Documenter(experiment_dir, experiment_name, {\"video name\": video_name}, True)\n",
    "    # Doc.comment(\"Check why there are still jumps in the contour.\")\n",
    "\n",
    "    print(f\"Processing {video_name}\")\n",
    "    frames = get_video_frames(f\"videos/{video_name}\")\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "    h, w = frames[0].shape\n",
    "    blank_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front = front_from_frames(frames[160], frames[161])\n",
    "        contours = contours_from_front(front)\n",
    "\n",
    "        width = max((len(contours) + 1) * 5, 15)\n",
    "        fig, ax = plt.subplots(\n",
    "            dpi=200,\n",
    "            figsize=(width, 5),\n",
    "            nrows=1,\n",
    "            ncols=len(contours) + 1,\n",
    "            width_ratios=[0.5] * len(contours) + [1],\n",
    "            squeeze=False,\n",
    "        )\n",
    "        ax[-1, len(contours)].imshow(blank_canvas)\n",
    "        ax[-1, len(contours)].set_title(\"Splines fit to Contour Lines\")\n",
    "        for j, contour in enumerate(contours):\n",
    "\n",
    "            # Doc.log(f\"Frame {i} - Contour raw {j}: {contour}\\n\")\n",
    "\n",
    "            contour = remove_duplicates(contour)\n",
    "\n",
    "            # Doc.log(f\"Frame {i} - Contour Unique {j}: {contour}\\n\")\n",
    "\n",
    "            contour = sample_contour(contour)\n",
    "\n",
    "            # Doc.log(f\"Frame {i} - Contour Downsampled {j}: {contour}\\n\")\n",
    "\n",
    "            intra_dists = np.diagonal(cdist(contour[:-1], contour[1:]))\n",
    "            mean_dist = np.mean(intra_dists)\n",
    "\n",
    "            contour = handle_outliers(contour)\n",
    "\n",
    "            # Doc.log(f\"Frame {i} - Contour filtered {j}: {contour}\\n\")\n",
    "\n",
    "            ax[-1, j].bar(range(len(intra_dists)), intra_dists, color=cmap(j))\n",
    "            ax[-1, j].set_title(f\"Point distances contour {j}\")\n",
    "            ax[-1, j].axhline(y=2 * mean_dist, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "            spline, _ = spline_from_contour(contour)\n",
    "\n",
    "            ax[-1, len(contours)].plot(spline[:, 0], spline[:, 1], linewidth=0.25, color=cmap(j))\n",
    "\n",
    "        fig.suptitle(f\"Analysis of Frame {i}\")\n",
    "        plt.savefig(f\"{experiment_dir}/{experiment_name}/frame_{i}.png\")\n",
    "        plt.close(fig)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14eb7af28645b448",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 9\n",
    "### Fit splines at each timestep.\n",
    "Based on the spline fitting procedure developed in experiment 8, create a vector field indicating the wavefront speed for the simplest video <i>no_shapes.mp4</i>.\n",
    "\n",
    "### Hypothesis\n",
    "The code will be specifically tailored to this video, so it should work quite well.\n",
    "\n",
    "### Results\n",
    "\n",
    "16.05.2024:\n",
    "- I did not find a good way to obtain the speed, but the directional field already looks decent. \n",
    "\n",
    "20.05.2024:\n",
    "- I updated the thresholding in <i>front_from_frames</i> and now the normals are more accurate.\n",
    "\n",
    "26.05.2024:\n",
    "- Optimizing contours using a version of the 2-opt algorithm for the TSP problem made the results worse by alot."
   ]
  },
  {
   "cell_type": "code",
   "id": "b34a7f25500baf4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.frame_processing import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.visualization import dfki_cmap, dist_to_idx\n",
    "from src.utils.documentation import Documenter\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63086f3caa56b83c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.utils.frame_processing import _orient_normal\n",
    "\n",
    "VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "VIDEO_NAMES = [\"vertical_lines.mp4\"]\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    params = {\"video name\": video_name, \"threshold\": 25, \"min_length\": 5}\n",
    "\n",
    "    video_title = video_name.split(\".\")[0]\n",
    "\n",
    "    experiment_dir = \"results/experiment9\"\n",
    "    experiment_name = f\"{video_title}_normal_orientation_debug\"\n",
    "\n",
    "    Doc = Documenter(experiment_dir, experiment_name, params, True)\n",
    "    Doc.comment(\"Trying to correct the orientation of the normal vectors in V3 of the pipeline.\")\n",
    "\n",
    "    frames = get_video_frames(f\"videos/{params['video name']}\")\n",
    "\n",
    "    fig = plt.figure(dpi=400)\n",
    "\n",
    "    for i, _ in tqdm(enumerate(frames[1:-2])):\n",
    "        front = front_from_frames(\n",
    "            frames[i - 1], frames[i], frames[i + 1], threshold=params[\"threshold\"]\n",
    "        )\n",
    "        front_next = front_from_frames(\n",
    "            frames[i], frames[i + 1], frames[i + 2], threshold=params[\"threshold\"]\n",
    "        )\n",
    "\n",
    "        contours = contours_from_front(front, min_length=params[\"min_length\"])\n",
    "        contours_next = contours_from_front(front_next, min_length=params[\"min_length\"])\n",
    "\n",
    "        for j, contour in enumerate(contours):\n",
    "\n",
    "            spline, normals = spline_from_contour(contour)\n",
    "\n",
    "            plt.plot(spline[:, 0], spline[:, 1], linewidth=0.25, color=\"green\")\n",
    "            Doc.log(f\"Frame {i} - Spline {j}: {np.round(spline, 2)}\")\n",
    "            Doc.log(f\"Frame {i} - Normals {j}: {normals}\")\n",
    "            Doc.log(\"Sign values: \")\n",
    "            for point, normal in zip(spline, normals):\n",
    "\n",
    "                min_dist, _, sign_x = dist_to_nearest(point, contours_next)\n",
    "                Doc.log(f\"{sign_x}\")\n",
    "\n",
    "                if min_dist == np.inf or min_dist > 15:\n",
    "                    continue\n",
    "\n",
    "                normal = _orient_normal(normal, sign_x)\n",
    "\n",
    "                plt.quiver(\n",
    "                    point[0],\n",
    "                    point[1],\n",
    "                    normal[0],\n",
    "                    normal[1],\n",
    "                    color=dfki_cmap(dist_to_idx(min_dist)),\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1 / (min_dist + 10e-6),\n",
    "                    width=0.001,\n",
    "                )\n",
    "            Doc.newline()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Vector Field of Reaction Front for {params['video name']}\")\n",
    "    plt.savefig(f\"{experiment_dir}/{experiment_name}/{video_title}_vector_field.png\")\n",
    "    plt.close(fig)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44fb62dc5b80860c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "visualize all steps of the current video processing pipeline for my presentation </font>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from src.utils.visualization import plot_simple\n",
    "from src.utils.frame_processing import (\n",
    "    binarize,\n",
    "    apply_morphology,\n",
    "    process_contour,\n",
    "    contours_from_front,\n",
    "    spline_from_contour,\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "fbab1aed58da0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af9df704e87d02b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "video_path = \"videos/no_shape.mp4\"\n",
    "frame_idx0 = 104\n",
    "frame_idx1 = 105\n",
    "img_dir = \"results/visualization/\"\n",
    "\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "frames = get_video_frames(video_path)\n",
    "\n",
    "frame_1 = frames[frame_idx1]\n",
    "frame_0 = frames[frame_idx0]\n",
    "\n",
    "plot_simple(frame_0, f\"frame_{frame_idx0}\", img_dir)\n",
    "plot_simple(frame_1, f\"frame_{frame_idx1}\", img_dir)\n",
    "\n",
    "# Threshold and binarize the images.\n",
    "binarize(frame_0, frame_1, 25)\n",
    "\n",
    "plot_simple(frame_0, f\"frame_{frame_idx0}_denoised\", img_dir)\n",
    "plot_simple(frame_1, f\"frame_{frame_idx1}_denoised\", img_dir)\n",
    "\n",
    "# Calculate the difference between two frames.\n",
    "front = frame_1 - frame_0\n",
    "\n",
    "plot_simple(front, \"front_difference\", img_dir)\n",
    "\n",
    "front = apply_morphology(front)\n",
    "\n",
    "plot_simple(front, \"front_morphological\", img_dir)\n",
    "\n",
    "h, w = front.shape\n",
    "contour_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "spline_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "arrow_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "contours = contours_from_front(front)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "ax1.imshow(contour_canvas, cmap=\"gray\")\n",
    "ax2.imshow(spline_canvas, cmap=\"gray\")\n",
    "ax3.imshow(arrow_canvas, cmap=\"gray\")\n",
    "\n",
    "for j, contour in enumerate(contours):\n",
    "\n",
    "    contour = process_contour(contour)\n",
    "\n",
    "    x = contour[:, 0]\n",
    "    y = contour[:, 1]\n",
    "\n",
    "    color_j = dfki_cmap(max(10 * j, 255))\n",
    "    ax1.plot(x, y, color=color_j)\n",
    "\n",
    "    spline, normal = spline_from_contour(contour)\n",
    "\n",
    "    ax2.plot(spline[:, 0], spline[:, 1], color=color_j)\n",
    "    ax3.plot(spline[:, 0], spline[:, 1], color=color_j)\n",
    "\n",
    "    plt.quiver(\n",
    "        spline[1:-1, 0],\n",
    "        spline[1:-1, 1],\n",
    "        normal[1:-1, 0],\n",
    "        normal[1:-1, 1],\n",
    "        color=color_j,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.01,\n",
    "        width=0.001,\n",
    "    )\n",
    "\n",
    "fig1.savefig(f\"{img_dir}/contours.png\")\n",
    "fig2.savefig(f\"{img_dir}/splines.png\")\n",
    "fig3.savefig(f\"{img_dir}/arrows.png\")\n",
    "\n",
    "plt.close(fig1)\n",
    "plt.close(fig2)\n",
    "plt.close(fig3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd6de3d7ebe4fbf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 10\n",
    "### Track tiny reaction fronts.\n",
    "In some videos such as <i>vertical_lines.mp4</i> some of the offsprings of the initial reaction front are very small and thus hard to track. The skeletonization als results in deformed front that deviate a lot from the smooth curved fronts that are observed in other parts of the video.\n",
    "\n",
    "21.05.2024:\n",
    "- I try out different types of image processing steps and denoising to improve the quality of small reaction fronts.\n",
    "\n",
    "### Results\n",
    "\n",
    "20.05.2024:\n",
    "- When the front passes through small gaps a part of it's trail is recognized as belonging to the front itself, which gives the front a rounded appearance."
   ]
  },
  {
   "cell_type": "code",
   "id": "15a2e4ec61f319c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology, filters, restoration"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcb900b88b13c769",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "frame0 = frames[110]\n",
    "frame1 = frames[111]\n",
    "\n",
    "frame0[frame0 > 35] = 255\n",
    "frame0[frame0 < 35] = 0\n",
    "frame1[frame1 > 35] = 255\n",
    "frame1[frame1 < 35] = 0\n",
    "\n",
    "front = morphology.opening(frame1 - frame0, morphology.disk(3))\n",
    "plt.imshow(frames[112], cmap=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "602b2d49ff3d67d9",
   "metadata": {
    "collapsed": false
   },
   "source": "<font size=\"4\">Check out the effects of Sobel filtering on the front.</font>"
  },
  {
   "cell_type": "code",
   "id": "8dcf5ff911405c52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "edges = filters.sobel(frames[110])\n",
    "plt.imshow(edges, cmap=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33f6cc07a74245d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visualize areas of low contrast via local entropy"
   ]
  },
  {
   "cell_type": "code",
   "id": "65b594b27b5c698e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "entr_img = filters.rank.entropy(frames[110], morphology.disk(10))\n",
    "\n",
    "plt.imshow(entr_img, cmap=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3c88569cf850fa1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "denoised0 = restoration.denoise_tv_chambolle(frames[110], weight=0.5)\n",
    "denoised1 = restoration.denoise_tv_chambolle(frames[111], weight=0.5)\n",
    "\n",
    "plt.imshow(denoised1 - denoised0, cmap=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c86393d16dbb597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment 11\n",
    "### Overlay reaction front recording with fitted splines.\n",
    "We want to overlay the splines with images of the actual reaction to check whether the curve fits the reaction front.\n",
    "\n",
    "09.07.2024:\n",
    "- Adapt the experiment to work with version 3 of the pipeline.\n",
    "### Hypothesis\n",
    "The overlay will look decent but might contain some noise.\n",
    "\n",
    "\n",
    "### Results\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b61bc6a25dc0e662"
  },
  {
   "cell_type": "code",
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.frame_processing import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.visualization import dfki_cmap, dist_to_idx\n",
    "from src.utils.documentation import Documenter\n",
    "import os\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "610ce3297a96701e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "VIDEO_NAMES = [\"vertical_lines.mp4\"]\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    params = {\"video name\": video_name, \"threshold\": 25, \"min_length\": 5}\n",
    "\n",
    "    video_title = video_name.split(\".\")[0]\n",
    "\n",
    "    experiment_dir = \"results/experiment11\"\n",
    "    experiment_name = f\"experiment11_{video_title}_no_correction\"\n",
    "\n",
    "    Doc = Documenter(experiment_dir, experiment_name, params)\n",
    "    Doc.comment(\n",
    "        \"I wanted to have a reference experiment where I don't even attempt to correct the normals.\"\n",
    "    )\n",
    "\n",
    "    frames = get_video_frames(f\"videos/{params['video name']}\")\n",
    "    # frames are processed inplace by front_from_frames.\n",
    "    frames_raw = get_video_frames(f\"videos/{params['video name']}\")\n",
    "\n",
    "    # Initialize the front\n",
    "    h, w = frames[0].shape\n",
    "    for i, _ in tqdm(enumerate(frames[1:-2])):\n",
    "\n",
    "        fig = plt.figure(dpi=400)\n",
    "        plt.imshow(frames_raw[i], cmap=\"gray\")\n",
    "\n",
    "        front = front_from_frames(\n",
    "            frames[i - 1], frames[i], frames[i + 1], threshold=params[\"threshold\"]\n",
    "        )\n",
    "        front_next = front_from_frames(\n",
    "            frames[i], frames[i + 1], frames[i + 2], threshold=params[\"threshold\"]\n",
    "        )\n",
    "\n",
    "        contours = contours_from_front(front, min_length=params[\"min_length\"])\n",
    "        contours_next = contours_from_front(front_next, min_length=params[\"min_length\"])\n",
    "\n",
    "        for j, contour in enumerate(contours):\n",
    "\n",
    "            spline, normals = spline_from_contour(contour)\n",
    "\n",
    "            plt.plot(spline[:, 0], spline[:, 1], linewidth=0.25, color=dfki_cmap(250))\n",
    "\n",
    "            for point, normal in zip(spline, normals):\n",
    "                min_dist, _, sign = dist_to_nearest(point, contours_next)\n",
    "\n",
    "                if min_dist == np.inf:\n",
    "                    continue\n",
    "\n",
    "                plt.quiver(\n",
    "                    point[0],\n",
    "                    point[1],\n",
    "                    normal[0],\n",
    "                    normal[1],\n",
    "                    color=dfki_cmap(dist_to_idx(min_dist)),\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1 / (min_dist + 10e-6),\n",
    "                    width=0.0005,\n",
    "                )\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(experiment_dir + \"/\" + experiment_name + f\"/frame_{i}.png\")\n",
    "        plt.close(fig)"
   ],
   "id": "6decb072ccc6aa43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment 12\n",
    "### Plot front at various stages of processing.\n",
    "We want to view the front at various stages of processing to get a better understanding of the effects of the individual steps and how to improve them.\n",
    "\n",
    "09.07.2024:\n",
    "- I experimented with edge detection to improve the fit of the spline to the reaction front. I got some promising results on one non trivial frame\n",
    "  from vertical_lines.mp4. Based on this result I implement a version 3 of the image pipeline.\n",
    "### Results"
   ],
   "id": "493489442a3003d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils.frame_processing import (\n",
    "    _binarize,\n",
    "    _edges_from_frame,\n",
    "    _front_from_edges,\n",
    "    _apply_morphology,\n",
    ")\n",
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.visualization import plot_simple\n",
    "import os"
   ],
   "id": "2d9c0d4be72c86bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frame_index = 50\n",
    "video_name = \"no_shape\"\n",
    "video_path = f\"videos/{video_name}.mp4\"\n",
    "img_dir = f\"results/experiment12/{video_name}/{frame_index}/\"\n",
    "if not os.path.isdir(img_dir):\n",
    "    os.makedirs(img_dir)"
   ],
   "id": "81bb2308d63c2002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frames = get_video_frames(video_path)\n",
    "\n",
    "frame0 = frames[frame_index]\n",
    "frame1 = frames[frame_index + 1]\n",
    "frame2 = frames[frame_index + 2]\n",
    "\n",
    "plot_simple(frame0, f\"frame_{frame_index}_raw\", img_dir)\n",
    "plot_simple(frame1, f\"frame_{frame_index + 1}_raw\", img_dir)\n",
    "plot_simple(frame2, f\"frame_{frame_index + 2}_raw\", img_dir)\n",
    "\n",
    "_binarize(frame0)\n",
    "_binarize(frame1)\n",
    "_binarize(frame2)\n",
    "\n",
    "plot_simple(frame0, f\"frame_{frame_index}_b\", img_dir)\n",
    "plot_simple(frame1, f\"frame_{frame_index + 1}_b\", img_dir)\n",
    "plot_simple(frame2, f\"frame_{frame_index + 2}_b\", img_dir)\n",
    "\n",
    "edges0 = _edges_from_frame(frame0)\n",
    "edges1 = _edges_from_frame(frame1)\n",
    "edges2 = _edges_from_frame(frame2)\n",
    "\n",
    "plot_simple(edges0, f\"edges_{frame_index}\", img_dir)\n",
    "plot_simple(edges1, f\"edges_{frame_index + 1}\", img_dir)\n",
    "plot_simple(edges2, f\"edges_{frame_index + 2}\", img_dir)\n",
    "\n",
    "front = _front_from_edges(edges0, edges1, edges2)\n",
    "plot_simple(front, \"front_raw\", img_dir)\n",
    "\n",
    "front = _apply_morphology(front)\n",
    "plot_simple(front, \"front_morphological\", img_dir)"
   ],
   "id": "d7ae1528397a5e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment 13\n",
    "### Plot new outlier handling\n",
    "After researching how OpenCVs finds contours I became more confident in the feasibility of outlier handling. I changed the outlier handling functionality such that no points are discarded anymore.\n",
    "\n",
    "### Hypothesis\n",
    "This will make the splines fit better to small reaction fronts.\n",
    "\n",
    "### Results\n",
    "- 06.08.2024: The fit is indeed better, my hypothesis has stood the test. However I noticed a bug in _orient_normals\n",
    "- 06.08.2024: I successfully solved the normal direction problem."
   ],
   "id": "804c758fdfec16f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:32:30.420058Z",
     "start_time": "2024-08-14T14:32:29.591941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils.video_handling import get_video_frames\n",
    "from src.utils.frame_processing import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.visualization import dfki_cmap, dist_to_idx\n",
    "from src.utils.documentation import Documenter\n",
    "import os\n",
    "import numpy as np"
   ],
   "id": "358945c4a693edca",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T15:06:23.270647Z",
     "start_time": "2024-08-14T14:32:32.358380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    params = {\"video name\": video_name, \"threshold\": 25}\n",
    "\n",
    "    video_title = video_name.split(\".\")[0]\n",
    "\n",
    "    experiment_dir = \"results/experiment13\"\n",
    "    experiment_name = f\"{video_title}_normal_or_vec_v7\"\n",
    "\n",
    "    Doc = Documenter(experiment_dir, experiment_name, params, True)\n",
    "    Doc.comment(\n",
    "        \"Test whether I fixed the issue with the contours being extra long due to sampling issues. Use old 8eb021 code for sampling and add keep rate.\"\n",
    "    )\n",
    "\n",
    "    frames = get_video_frames(f\"videos/{params['video name']}\")\n",
    "\n",
    "    fig = plt.figure(dpi=400)\n",
    "\n",
    "    for i, _ in tqdm(enumerate(frames[1:-2])):\n",
    "        front = front_from_frames(\n",
    "            frames[i - 1], frames[i], frames[i + 1], threshold=params[\"threshold\"]\n",
    "        )\n",
    "        front_next = front_from_frames(\n",
    "            frames[i], frames[i + 1], frames[i + 2], threshold=params[\"threshold\"]\n",
    "        )\n",
    "\n",
    "        contours = contours_from_front(front)\n",
    "        contours_next = contours_from_front(front_next)\n",
    "\n",
    "        for j, contour in enumerate(contours):\n",
    "\n",
    "            spline, normals = spline_from_contour(contour)\n",
    "\n",
    "            plt.plot(spline[:, 0], spline[:, 1], linewidth=0.25, color=\"#6BBEA1\")\n",
    "\n",
    "            for point, normal in zip(spline, normals):\n",
    "                vec_nearest = vec_to_nearest(point, contours_next)\n",
    "\n",
    "                if vec_nearest is None:\n",
    "                    continue\n",
    "\n",
    "                dist = np.linalg.norm(vec_nearest)\n",
    "\n",
    "                if dist > 15:\n",
    "                    continue\n",
    "\n",
    "                normal = get_direction(normal, vec_nearest)\n",
    "\n",
    "                plt.quiver(\n",
    "                    point[0],\n",
    "                    point[1],\n",
    "                    normal[0],\n",
    "                    normal[1],\n",
    "                    color=dfki_cmap(dist_to_idx(dist)),\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1 / (dist + 10e-6),\n",
    "                    width=0.0005,\n",
    "                )\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Vector Field of Reaction Front for {params['video name']}\")\n",
    "    plt.savefig(f\"{experiment_dir}/{experiment_name}/{video_title}_vector_field.png\")\n",
    "    plt.close(fig)"
   ],
   "id": "42659af8bdce5d11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 30_micron_gap.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [00:05, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comb_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [05:25,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [05:59,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing double_c_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228it [05:34,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [04:45,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing no_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [04:53,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small_holes.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248it [00:14, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing standard.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [00:06, 31.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vertical_lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [05:33,  1.77s/it]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment 14\n",
    "### Perform data analysis on pipeline output\n",
    "I want to analyze the data file generated by the image pipeline to see whether I can confirm that there are region with different speed.\n",
    "### Hypothesis\n",
    "I will get a bimodal histogram for the video comb_shape.mp4\n",
    "\n",
    "### Results\n"
   ],
   "id": "3683826494573cb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T07:02:59.106032Z",
     "start_time": "2024-08-19T07:02:56.099295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils.pipeline import pipeline\n",
    "from src.utils.video_handling import get_video_frames"
   ],
   "id": "1dc97aa91bb3d592",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T07:07:42.383600Z",
     "start_time": "2024-08-19T07:07:09.371803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "VIDEO_NAMES = [\"vertical_lines.mp4\"]\n",
    "\n",
    "PIXEL_SIZE = 0.0201  # (mm / pixel)\n",
    "FPS = 100_000  # (frames / s)\n",
    "MM_TO_METER = 1 / 1000  # (m / mm)\n",
    "SCALE = 1 / 2.44  # (mp4 video was scaled by factor 2.44)\n",
    "\n",
    "experiment_dir = \"results/experiment14\"\n",
    "\n",
    "if not os.path.isdir(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    video_title = video_name.split(\".\")[0]\n",
    "\n",
    "    frames = get_video_frames(f\"videos/{video_name}\")\n",
    "    data = pipeline(frames)\n",
    "\n",
    "    speeds = np.round(data[:, 6] * PIXEL_SIZE * FPS * MM_TO_METER * SCALE, 2)\n",
    "    speeds = speeds[speeds < 25]\n",
    "\n",
    "    max_speed = np.max(speeds)\n",
    "    min_speed = np.min(speeds)\n",
    "\n",
    "    bins = np.linspace(min_speed, max_speed, 101)\n",
    "    hist, bin_edges = np.histogram(speeds, bins=bins)\n",
    "\n",
    "    plt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), color=(0 / 255, 41 / 255, 123 / 255))\n",
    "    plt.xlabel(\"Speed (m/s)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of Reaction Speed {video_title}\")\n",
    "    plt.savefig(f\"{experiment_dir}/{video_title}_histogram.png\")\n",
    "    plt.close()"
   ],
   "id": "205e083354381ffd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vertical_lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running image pipeline:  99%|\u001b[38;2;109;190;160m█████████▉\u001b[0m| 188/189 [00:32<00:00,  5.84 frames/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb3701e05f003cf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ee557716db983d61",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
