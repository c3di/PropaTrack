{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24eb56a7c47fcd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "### Notebook to run experiments on the data supplied by the chair of functional materials at Saarland University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70237172f5fc33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 1\n",
    "### Run RAFT optical flow model on the data\n",
    "In this experiment we will run the RAFT optical flow model on the data to see how well it performs compared to classical optical flow methods.\n",
    "\n",
    "\n",
    "### Results\n",
    "RAFT tracks the reaction front quite well. The reaction front is visible in the optical flow images but there is also a lot of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c345de85fb8145",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from raft.utils import get_video_frames, plot_frames, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe78f1d47b1f36",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_path = \"videos/double_c_shape.mp4\"\n",
    "frames = get_video_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f8cbec9819aa6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the numbers in frames to see different images.\n",
    "img_batch = torch.stack([frames[50], frames[51]])\n",
    "plot_frames(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9a9e2fcb866d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.models.optical_flow.raft import Raft_Large_Weights\n",
    "\n",
    "model = raft_large(weights=Raft_Large_Weights.C_T_V2)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cb202f1496af2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img1_batch: torch.Tensor = preprocess(torch.stack([frames[130], frames[220]]))\n",
    "img2_batch: torch.Tensor = preprocess(torch.stack([frames[131], frames[221]]))\n",
    "\n",
    "plot_frames(img1_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d9556dadca2ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img1_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4e2c8fc9adcf3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_flows: torch.Tensor = model(img1_batch, img2_batch)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69928f67bbbff1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import flow_to_image\n",
    "\n",
    "flow_images = flow_to_image(predicted_flows)\n",
    "\n",
    "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
    "img1_batch = (img1_batch + 1.0) / 2.0\n",
    "\n",
    "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_images)]\n",
    "plot_frames(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8ce6a",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "### Obtain reaction front from difference between frames\n",
    "In this experiment we will see how well we can obtain the reaction front by simply taking the difference between frames. We set pixels with very small and large differences between two frames to zero, as they seem to be noise. We also binarize the image to make the front more visible. For further denoising we apply a morphological operation called <i>Opening</i>, which removes all white spots smaller than a certain size. Mathematical morphology in the context of image processing deals with analysing and modifying structures in an image based on their geometrical properties. \n",
    "\n",
    "\n",
    "### Results\n",
    "The reaction front is actually quite visible in the difference between frames. This is a very simple method to obtain the reaction front.  The method also seems to outperform the old optical flow method in terms of noise and also speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cabb1949635f5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9de882a17a2f97",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    frames = get_video_frames(video_path)\n",
    "    dir_name = \"results/experiment2/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    video_writer = cv2.VideoWriter(dir_name + video_name, fourcc, 10.0, (width, height))\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i + 1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "\n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "\n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(flow)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e62ffc9c4bbb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 3\n",
    "### Track the reaction fronts through the frames\n",
    "In this experiment we will try to track the reaction front and its offsprings throughout the frames. Since the images will be binarized we can use local connectivity of the pixels as a criterion for tracking. \n",
    "\n",
    "### Results\n",
    "Segmenting the reaction front and it's offspring works quite good on a per frame basis, but it's difficult to track the different fronts throughout the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551152f2f46d43b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology, measure\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388951942084d30",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "props_old = None\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    frames = get_video_frames(video_path)\n",
    "    video_writer = cv2.VideoWriter(\n",
    "        \"results/experiment3/\" + video_name, fourcc, 10.0, (width, height)\n",
    "    )\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        flow = frames[i + 1] - frames[i]\n",
    "        flow = flow.astype(np.uint8)\n",
    "\n",
    "        # Threshold and binarize the image.\n",
    "        flow[flow > 180] = 0\n",
    "        flow[flow < 20] = 0\n",
    "        flow[flow > 0] = 255\n",
    "\n",
    "        # Apply morphological opening.\n",
    "        flow = morphology.opening(flow, footprint)\n",
    "\n",
    "        # Label the connected components.\n",
    "        labeled_flow, num_labels = measure.label(flow, return_num=True)\n",
    "        labeled_flow = cv2.cvtColor(labeled_flow.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        for j in range(1, num_labels):\n",
    "            color = cmap(j)\n",
    "            color_array = (np.array(color[:-1]) * 255).astype(np.uint8)\n",
    "            flow = np.where(labeled_flow == j, color_array, flow)\n",
    "\n",
    "        video_writer.write(flow)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad77b56f4d1076",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 4\n",
    "### Apply optical flow to the binarized video frames\n",
    "In this experiment we will apply an optical flow algorithm to the binarized videos to get a vector field describing the speed of the reaction front at each point in time.\n",
    "\n",
    "04.05.2024:\n",
    "- Try adding skeletonization before calculating the optical flow and use opencv's calcOpticalFlowFarneback function.\n",
    "\n",
    "05.05.2024:\n",
    "- I tried playing around with the parameters of the algorithm like *poly_sigma* and *poly_n*.\n",
    "<br><br>\n",
    "\n",
    "### Hypothesis\n",
    "Since the frames are almost noise free I expect the optical flow algorithm to perform quite well on the data.\n",
    "<br><br>\n",
    "\n",
    "### Results\n",
    "The algorithm fails spectacularly, seemingly tracking the reaction fronts but also causing seemingly random splashes of gray tones.\n",
    "\n",
    "04.05.2024:\n",
    "- After applying skeletonization there are no splashes anymore, but there still is noise and the flow seems to be too thick.\n",
    "\n",
    "05.05.2024:\n",
    "- Modifying the parameters doesn't improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4214c826a351e81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import get_video_frames\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5720011bce3579c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "footprint = morphology.disk(3)\n",
    "\n",
    "cmap = plt.get_cmap(\"plasma\")\n",
    "props_old = None\n",
    "\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    print(f\"Generating flow for {video_name}\")\n",
    "    video_path = \"videos/\" + video_name\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    frames = get_video_frames(video_path)\n",
    "\n",
    "    dir_name = \"results/experiment4_farneback_poly_sigma_0_2_poly_n_3/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    video_writer = cv2.VideoWriter(dir_name + video_name, fourcc, 10.0, (width, height))\n",
    "\n",
    "    for i, _ in tqdm(enumerate(frames[1:-1])):\n",
    "        # Calculate the difference between two frames.\n",
    "        diff_flow0 = (frames[i + 1] - frames[i]).astype(np.uint8)\n",
    "        diff_flow1 = (frames[i] - frames[i - 1]).astype(np.uint8)\n",
    "\n",
    "        # Threshold and binarize the image.\n",
    "        diff_flow0[diff_flow0 > 180] = 0\n",
    "        diff_flow0[diff_flow0 < 20] = 0\n",
    "        diff_flow0[diff_flow0 > 0] = 255\n",
    "\n",
    "        diff_flow1[diff_flow1 > 180] = 0\n",
    "        diff_flow1[diff_flow1 < 20] = 0\n",
    "        diff_flow1[diff_flow1 > 0] = 255\n",
    "\n",
    "        # Apply morphological opening.\n",
    "        diff_flow0 = morphology.opening(diff_flow0, footprint)\n",
    "        diff_flow1 = morphology.opening(diff_flow1, footprint)\n",
    "\n",
    "        diff_flow0 = morphology.skeletonize(diff_flow0)\n",
    "        diff_flow1 = morphology.skeletonize(diff_flow1)\n",
    "\n",
    "        diff_flow0 = np.where(diff_flow0 == 1, 255, 0).astype(np.uint8)\n",
    "        diff_flow1 = np.where(diff_flow1 == 1, 255, 0).astype(np.uint8)\n",
    "\n",
    "        flow_field = cv2.calcOpticalFlowFarneback(\n",
    "            diff_flow0, diff_flow1, None, 0.5, 3, 15, 3, 3, 0.2, 0\n",
    "        )\n",
    "\n",
    "        flow = np.sqrt(flow_field[:, :, 0] ** 2 + flow_field[:, :, 1] ** 2).astype(np.uint8)\n",
    "        flow = cv2.cvtColor(flow, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        video_writer.write(flow)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107f6d6727dcd85",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Experiment 5\n",
    "### Apply skeletonization to the binarized video frames\n",
    "In this experiment we will apply skeletonization to the binarized videos to get a more sharp representation of the reaction front.\n",
    "\n",
    "### Hypothesis\n",
    "Will look like the previous experiments but with thin lines instead of blobs.\n",
    "\n",
    "### Results\n",
    "Hypothesis turned out to be correct, there is still some noise present which I might be able to remove with more morphological operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cb4aaf8fd641e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "id": "92a0fb52b60bd329",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:22:26.450233Z",
     "start_time": "2024-06-16T18:22:25.892654Z"
    }
   },
   "source": [
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from utils.video_handling import setup_experiment\n",
    "from utils.frame_processing import front_from_frames\n",
    "import cv2\n",
    "import os"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Bindings generation error. Submodule name should always start with a parent module name. Parent name: cv2.cv2. Submodule name: cv2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m morphology\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvideo_handling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m setup_experiment\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframe_processing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m front_from_frames\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\DFKI\\experiments\\utils\\video_handling.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tuple\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cv2\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_video_frames\u001B[39m(video_path: \u001B[38;5;28mstr\u001B[39m, grayscale: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39marray:\n",
      "\u001B[1;31mImportError\u001B[0m: Bindings generation error. Submodule name should always start with a parent module name. Parent name: cv2.cv2. Submodule name: cv2"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8ceee842f2d95",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "footprint = morphology.disk(3)\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    video_writer, frames = setup_experiment(video_name, \"results/experiment5_low_threshold_15/\")\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front = front_from_frames(frames[i], frames[i + 1], footprint)\n",
    "        front = cv2.cvtColor(front, cv2.COLOR_GRAY2BGR)\n",
    "        video_writer.write(front)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d54236e984a9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 6\n",
    "### Plot evolution of reaction front on a single canvas\n",
    "In this experiment we will plot the reaction front at each time step on a single canvas to get an overview over its evolution in time from a single image.\n",
    "\n",
    "### Results \n",
    "Fronts are clearly visible in most images. Only the videos with low resolutions and the one with the dots yield unclear images\n",
    "\n",
    "20.05.2024\n",
    "- It seems like v2 of the <i>front_from_frames</i> works better on images with small structures, while v1 works better on images with large structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53a4744913f28c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T09:08:29.318017Z",
     "start_time": "2024-06-06T09:08:27.399606Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6ae4d69f61b9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T09:09:28.372410Z",
     "start_time": "2024-06-06T09:08:30.796814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "footprint = morphology.disk(3)\n",
    "for video_name in os.listdir(\"videos/\"):\n",
    "    frames = get_video_frames(\"videos/\" + video_name)\n",
    "    # Initialize the front\n",
    "    h, w = frames[0].shape\n",
    "    front = np.zeros((h, w), dtype=np.uint8)\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front += front_from_frames(frames[i], frames[i + 1], threshold=35)\n",
    "    fig = plt.figure(dpi=200)\n",
    "    plt.imshow(front)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Reaction Front Progression - {}\".format(video_name))\n",
    "    result_dir = \"results/experiment6_v2_tl_35/\"\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    plt.savefig(result_dir + video_name + \".png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb400dd012591c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 7\n",
    "### Calculate distance travelled by reaction front\n",
    "In this experiment we will calculate the distance travelled by the reaction fronts between two frames by fitting contour lines to at each timestep and calculating their distance.\n",
    "\n",
    "### Results\n",
    "Fronts are clearly visible in most images. Only the videos with low resolutions and the one with the dots yield unclear images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55da7eaa3c49a32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed089ce7d29c4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/double_c_shape.mp4\")\n",
    "footprint = morphology.disk(3)\n",
    "# Initialize the front\n",
    "h, w = frames[0].shape\n",
    "front = np.zeros((h, w), dtype=np.uint8)\n",
    "fronts = []\n",
    "for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "    front = front_from_frames(frames[i], frames[i + 1], footprint)\n",
    "    fronts.append(front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32758716d4e70d55",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200)\n",
    "image = fronts[11]\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Contour Lines of Reaction Front\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c263b3a5cf13e",
   "metadata": {
    "collapsed": false
   },
   "source": "Calculate the average distance between the two fronts by determining for each pixel on front 1 the closest pixel on front 2 and then taking the average of these distances."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff090d9490f15b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "distances = cdist(np.squeeze(contours[0]), np.squeeze(contours[1]))\n",
    "print(f\"Average Distance: {np.mean(np.min(distances, axis=0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065cf8312f59f71",
   "metadata": {
    "collapsed": false
   },
   "source": "Now we use scipy to fit splines to the contour lines and plot them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47c09250fd44d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, indices_unique = np.unique(contours[0], axis=0, return_index=True)\n",
    "contour0 = contours[0][np.sort(indices_unique), :]\n",
    "\n",
    "X0 = np.squeeze(contour0)[::5, 0]\n",
    "Y0 = np.squeeze(contour0)[::5, 1]\n",
    "\n",
    "X1 = np.squeeze(contours[1])[::20, 0]\n",
    "Y1 = np.squeeze(contours[1])[::20, 1]\n",
    "\n",
    "tck0, u0 = interpolate.splprep([X0, Y0], s=1)\n",
    "tck1, u1 = interpolate.splprep([X1, Y1], s=1)\n",
    "\n",
    "out0 = np.array(interpolate.splev(u0, tck0)).T\n",
    "out1 = np.array(interpolate.splev(u1, tck1)).T\n",
    "\n",
    "tangent0 = interpolate.splev(u0, tck0, der=1)\n",
    "tangent1 = interpolate.splev(u1, tck1, der=1)\n",
    "\n",
    "normal0 = np.array([-tangent0[1], tangent0[0]]).T\n",
    "normal1 = np.array([-tangent1[1], tangent1[0]]).T\n",
    "\n",
    "normal0 = normal0 / np.linalg.norm(normal0, axis=1).reshape(-1, 1)\n",
    "normal1 = normal1 / np.linalg.norm(normal1, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa56a5fa85b0206",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(image)\n",
    "plt.plot(out0[:, 0], out0[:, 1], \"y\", linewidth=0.25)\n",
    "plt.plot(out1[:, 0], out1[:, 1], \"g\", linewidth=0.25)\n",
    "plt.quiver(\n",
    "    out0[:, 0],\n",
    "    out0[:, 1],\n",
    "    normal0[:, 0],\n",
    "    normal0[:, 1],\n",
    "    color=\"r\",\n",
    "    angles=\"xy\",\n",
    "    scale_units=\"xy\",\n",
    "    scale=0.05,\n",
    "    width=0.001,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Splines fit to Contour Lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85585d9f53eb8532",
   "metadata": {
    "collapsed": false
   },
   "source": "Calculate the average distance between the two fronts by determining for each pixel on front 1 the closest pixel on front 2 and then taking the average of these distances. Since the splines are sparser than the original reaction front it is to be expected that the average distance is slightly larger."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c169e8ea3489f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spline0 = np.array(out0).T\n",
    "spline1 = np.array(out1).T\n",
    "distances = cdist(spline0, spline1)\n",
    "print(f\"Average Distance: {np.mean(np.min(distances, axis=0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd40d8a1babcc32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 8\n",
    "### Fit splines at each timestep.\n",
    "In this experiment we will try to fit splines to the reaction front at every timestep and evaluate how well the splines fit the front, especially when it comes to fitting small offsprings of the\n",
    "front and noise.\n",
    "\n",
    "11.05.2024:\n",
    "- Sometimes the spline does not follow the contour by connecting each point to its neighbor, but rather jumps to another point father away on the contour. I calculated the distance of each point on the contour to its neighbour in the contour array and plotted a histogram of these distances. The histogram indicates that usually the jump occurs at the last point in the array.\n",
    "- I was able to get rid of many (but not all) of these points by just removing the last point of the contour array. This is more of a hack, but it works for now.\n",
    "\n",
    "13.05.2024:\n",
    "- Using cv2.CHAIN_APPROX_NONE for the contours prevents situations where large jumps in the distances occur due to optimization of the contour.\n",
    "\n",
    "05.06.2023:\n",
    "- Use an updated scheme for handling outliers of the contour.\n",
    "\n",
    "### Hypothesis\n",
    "The splines will fit the front quite well but will have problems with small offsprings and noise.\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc058e06bdefd31",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T09:44:22.821841Z",
     "start_time": "2024-06-16T09:44:22.818116Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f18ec9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T09:46:32.235803Z",
     "start_time": "2024-06-16T09:44:26.074977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comb_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [02:05,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "VIDEO_NAMES = [\"comb_shape.mp4\"]\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    frames = get_video_frames(f\"videos/{video_name}\")\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    # Initialize the front\n",
    "    h, w = frames[0].shape\n",
    "    blank_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "    for i, _ in tqdm(enumerate(frames[:-1])):\n",
    "        front = front_from_frames(frames[i], frames[i + 1])\n",
    "        contours, _ = cv2.findContours(front, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        contours = [\n",
    "            np.squeeze(contour) for contour in contours if cv2.arcLength(contour, False) > 10\n",
    "        ]\n",
    "\n",
    "        width = max((len(contours) + 1) * 5, 15)\n",
    "        fig, ax = plt.subplots(\n",
    "            dpi=200,\n",
    "            figsize=(width, 5),\n",
    "            nrows=1,\n",
    "            ncols=len(contours) + 1,\n",
    "            width_ratios=[0.5] * len(contours) + [1],\n",
    "            squeeze=False,\n",
    "        )\n",
    "        ax[-1, len(contours)].imshow(blank_canvas)\n",
    "        ax[-1, len(contours)].set_title(\"Splines fit to Contour Lines\")\n",
    "        for j, contour in enumerate(contours):\n",
    "            # The contour wraps around the image, so we only take the first half.\n",
    "            _, indices_unique = np.unique(contour, axis=0, return_index=True)\n",
    "            contour = contour[np.sort(indices_unique), :]\n",
    "\n",
    "            steps = max(int(len(contour) * 0.025), 1)\n",
    "\n",
    "            if (len(contour) - 1) % steps == 0:\n",
    "                contour = contour[::steps]\n",
    "            else:\n",
    "                # Make sure the last point is included, so contours don't get cut off.\n",
    "                contour = np.append(contour[::steps], contour[-1:], axis=0)\n",
    "\n",
    "            intra_dists = np.diagonal(cdist(contour[:-1], contour[1:]))\n",
    "            mean_dist = np.mean(intra_dists)\n",
    "\n",
    "            outlier_dists = np.where(intra_dists > 3 * mean_dist)\n",
    "            if len(outlier_dists[0]) >= 1:\n",
    "                if outlier_dists[0][0] >= len(intra_dists) - 3:\n",
    "                    contour = contour[: -(len(intra_dists) - outlier_dists[0][0])]\n",
    "                else:\n",
    "                    contour_truncated = contour[: outlier_dists[0][0] + 1]\n",
    "                    contour_rest = contour[outlier_dists[0][-1] + 1 :]\n",
    "                    contour = np.concatenate((contour_rest[::-1], contour_truncated))\n",
    "\n",
    "            ax[-1, j].bar(range(len(intra_dists)), intra_dists, color=cmap(j))\n",
    "            ax[-1, j].set_title(f\"Point distances contour {j}\")\n",
    "            ax[-1, j].axhline(y=3 * mean_dist, color=\"r\", linestyle=\"--\")\n",
    "            X = contour[:, 0]\n",
    "            Y = contour[:, 1]\n",
    "\n",
    "            k = 1 if len(contour) < 10 else 3\n",
    "\n",
    "            tck, u = interpolate.splprep([X, Y], s=2, k=k)\n",
    "            out = np.array(interpolate.splev(u, tck)).T\n",
    "            ax[-1, len(contours)].plot(out[:, 0], out[:, 1], linewidth=0.25, color=cmap(j))\n",
    "\n",
    "        experiment_dir = f\"results/experiment8_{video_name.split('.')[0]}/\"\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        fig.suptitle(f\"Analysis of Frame {i}\")\n",
    "        plt.savefig(experiment_dir + f\"frame_{i}.png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb7af28645b448",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 9\n",
    "### Fit splines at each timestep.\n",
    "Based on the spline fitting procedure developed in experiment 8, create a vector field indicating the wavefront speed for the simplest video <i>no_shapes.mp4</i>.\n",
    "\n",
    "### Hypothesis\n",
    "The code will be specifically tailored to this video, so it should work quite well.\n",
    "\n",
    "### Results\n",
    "\n",
    "16.05.2024:\n",
    "- I did not find a good way to obtain the speed, but the directional field already looks decent. \n",
    "\n",
    "20.05.2024:\n",
    "- I updated the thresholding in <i>front_from_frames</i> and now the normals are more accurate.\n",
    "\n",
    "26.05.2024:\n",
    "- Optimizing contours using a version of the 2-opt algorithm for the TSP problem made the results worse by alot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34a7f25500baf4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T10:14:36.695573Z",
     "start_time": "2024-06-16T10:14:36.691136Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import (\n",
    "    front_from_frames,\n",
    "    process_contour,\n",
    "    contours_from_front,\n",
    "    spline_from_contour,\n",
    "    dist_to_nearest,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.visualization import dfki_cmap, dist_to_idx\n",
    "from utils.documentation import Documenter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63086f3caa56b83c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T10:21:45.714621Z",
     "start_time": "2024-06-16T10:15:50.019210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 30_micron_gap.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [00:01, 83.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comb_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197it [00:59,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:48,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing double_c_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [00:54,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [00:44,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing no_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [00:38,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small_holes.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [00:03, 80.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing standard.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [00:01, 128.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vertical_lines.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [01:12,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "# VIDEO_NAMES = [\"c_shape.mp4\", \"vertical_lines.mp4\"]\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    params = {\"video name\": video_name, \"threshold\": 25, \"min_length\": 5, \"version\": \"v2\"}\n",
    "\n",
    "    video_title = video_name.split(\".\")[0]\n",
    "\n",
    "    experiment_dir = \"results/experiment9\"\n",
    "    experiment_name = f\"{video_title}_spline_length\"\n",
    "\n",
    "    Doc = Documenter(experiment_dir, experiment_name, params, True)\n",
    "    Doc.comment(\"I want to check how many points the spline has for each contour.\")\n",
    "\n",
    "    frames = get_video_frames(f\"videos/{params['video name']}\")\n",
    "\n",
    "    fig = plt.figure(dpi=400)\n",
    "\n",
    "    for i, _ in tqdm(enumerate(frames[:-2])):\n",
    "        front = front_from_frames(frames[i], frames[i + 1], threshold=params[\"threshold\"])\n",
    "        front_next = front_from_frames(frames[i + 1], frames[i + 2], threshold=params[\"threshold\"])\n",
    "\n",
    "        contours = contours_from_front(front, min_length=params[\"min_length\"])\n",
    "        contours_next = contours_from_front(front_next, min_length=params[\"min_length\"])\n",
    "\n",
    "        for j, contour in enumerate(contours):\n",
    "\n",
    "            contour = process_contour(contour)\n",
    "\n",
    "            spline, normals = spline_from_contour(contour)\n",
    "\n",
    "            plt.plot(spline[:, 0], spline[:, 1], linewidth=0.25, color=\"green\")\n",
    "            Doc.log(f\"Frame {i} Contour {j} length of spline: {len(spline)}\")\n",
    "\n",
    "            for point, normal in zip(spline, normals):\n",
    "\n",
    "                min_dist = dist_to_nearest(point, contours_next)\n",
    "\n",
    "                plt.quiver(\n",
    "                    point[0],\n",
    "                    point[1],\n",
    "                    normal[0],\n",
    "                    normal[1],\n",
    "                    color=dfki_cmap(dist_to_idx(min_dist)),\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1 / (min_dist + 10e-6),\n",
    "                    width=0.001,\n",
    "                )\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Vector Field of Reaction Front for {params['video name']}\")\n",
    "    plt.savefig(experiment_dir + \"/\" + experiment_name + f\"/{video_title}_vector_field.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb62dc5b80860c",
   "metadata": {
    "collapsed": false
   },
   "source": "<font size=\"4\"> Visualize all steps of the current video processing pipeline for my presentation </font>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from utils.visualization import plot_simple\n",
    "from utils.frame_processing import (\n",
    "    binarize,\n",
    "    apply_morphology,\n",
    "    process_contour,\n",
    "    contours_from_front,\n",
    "    spline_from_contour,\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "fbab1aed58da0e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9df704e87d02b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_path = \"videos/no_shape.mp4\"\n",
    "frame_idx0 = 104\n",
    "frame_idx1 = 105\n",
    "img_dir = \"results/visualization/\"\n",
    "\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "frames = get_video_frames(video_path)\n",
    "\n",
    "frame_1 = frames[frame_idx1]\n",
    "frame_0 = frames[frame_idx0]\n",
    "\n",
    "plot_simple(frame_0, f\"frame_{frame_idx0}\", img_dir)\n",
    "plot_simple(frame_1, f\"frame_{frame_idx1}\", img_dir)\n",
    "\n",
    "# Threshold and binarize the images.\n",
    "binarize(frame_0, frame_1, 25)\n",
    "\n",
    "plot_simple(frame_0, f\"frame_{frame_idx0}_denoised\", img_dir)\n",
    "plot_simple(frame_1, f\"frame_{frame_idx1}_denoised\", img_dir)\n",
    "\n",
    "# Calculate the difference between two frames.\n",
    "front = frame_1 - frame_0\n",
    "\n",
    "plot_simple(front, \"front_difference\", img_dir)\n",
    "\n",
    "front = apply_morphology(front)\n",
    "\n",
    "plot_simple(front, \"front_morphological\", img_dir)\n",
    "\n",
    "h, w = front.shape\n",
    "contour_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "spline_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "arrow_canvas = np.zeros((h, w), dtype=np.uint8)\n",
    "contours = contours_from_front(front)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "ax1.imshow(contour_canvas, cmap=\"gray\")\n",
    "ax2.imshow(spline_canvas, cmap=\"gray\")\n",
    "ax3.imshow(arrow_canvas, cmap=\"gray\")\n",
    "\n",
    "for j, contour in enumerate(contours):\n",
    "\n",
    "    contour = process_contour(contour)\n",
    "\n",
    "    x = contour[:, 0]\n",
    "    y = contour[:, 1]\n",
    "\n",
    "    color_j = dfki_cmap(max(10 * j, 255))\n",
    "    ax1.plot(x, y, color=color_j)\n",
    "\n",
    "    spline, normal = spline_from_contour(contour)\n",
    "\n",
    "    ax2.plot(spline[:, 0], spline[:, 1], color=color_j)\n",
    "    ax3.plot(spline[:, 0], spline[:, 1], color=color_j)\n",
    "\n",
    "    plt.quiver(\n",
    "        spline[1:-1, 0],\n",
    "        spline[1:-1, 1],\n",
    "        normal[1:-1, 0],\n",
    "        normal[1:-1, 1],\n",
    "        color=color_j,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.01,\n",
    "        width=0.001,\n",
    "    )\n",
    "\n",
    "fig1.savefig(f\"{img_dir}/contours.png\")\n",
    "fig2.savefig(f\"{img_dir}/splines.png\")\n",
    "fig3.savefig(f\"{img_dir}/arrows.png\")\n",
    "\n",
    "plt.close(fig1)\n",
    "plt.close(fig2)\n",
    "plt.close(fig3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6de3d7ebe4fbf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment 10\n",
    "### Track tiny reaction fronts.\n",
    "In some videos such as <i>vertical_lines.mp4</i> some of the offsprings of the initial reaction front are very small and thus hard to track. The skeletonization als results in deformed front that deviate a lot from the smooth curved fronts that are observed in other parts of the video.\n",
    "\n",
    "21.05.2024:\n",
    "- I try out different types of image processing steps and denoising to improve the quality of small reaction fronts.\n",
    "\n",
    "### Results\n",
    "\n",
    "20.05.2024:\n",
    "- When the front passes through small gaps a part of it's trail is recognized as belonging to the front itself, which gives the front a rounded appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2e4ec61f319c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology, filters, restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb900b88b13c769",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "frame0 = frames[110]\n",
    "frame1 = frames[111]\n",
    "\n",
    "frame0[frame0 > 35] = 255\n",
    "frame0[frame0 < 35] = 0\n",
    "frame1[frame1 > 35] = 255\n",
    "frame1[frame1 < 35] = 0\n",
    "\n",
    "front = morphology.opening(frame1 - frame0, morphology.disk(3))\n",
    "plt.imshow(frames[112], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b2d49ff3d67d9",
   "metadata": {
    "collapsed": false
   },
   "source": "<font size=\"4\">Check out the effects of Sobel filtering on the front.</font>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf5ff911405c52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "edges = filters.sobel(frames[110])\n",
    "plt.imshow(edges, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6cc07a74245d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visualize areas of low contrast via local entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b594b27b5c698e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "entr_img = filters.rank.entropy(frames[110], morphology.disk(10))\n",
    "\n",
    "plt.imshow(entr_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c88569cf850fa1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = get_video_frames(\"videos/vertical_lines.mp4\")\n",
    "\n",
    "denoised0 = restoration.denoise_tv_chambolle(frames[110], weight=0.5)\n",
    "denoised1 = restoration.denoise_tv_chambolle(frames[111], weight=0.5)\n",
    "\n",
    "plt.imshow(denoised1 - denoised0, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c86393d16dbb597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment 11\n",
    "### Overlay reaction front recording with fitted splines.\n",
    "We want to overlay the splines with images of the actual reaction to check whether the curve fits the reaction front.\n",
    "\n",
    "\n",
    "### Hypothesis\n",
    "The overlay will look decent but might contain some noise.\n",
    "\n",
    "\n",
    "### Results\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b61bc6a25dc0e662"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils.video_handling import get_video_frames\n",
    "from utils.frame_processing import front_from_frames, process_contour, contours_from_front\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.visualization import dfki_cmap, dist_to_idx\n",
    "from utils.documentation import Documenter\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T06:48:55.979283Z",
     "start_time": "2024-06-16T06:48:52.632664Z"
    }
   },
   "id": "610ce3297a96701e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comb_shape.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197it [01:53,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# VIDEO_NAMES = os.listdir(\"videos/\")\n",
    "VIDEO_NAMES = [\"comb_shape.mp4\"]\n",
    "\n",
    "for video_name in VIDEO_NAMES:\n",
    "    print(f\"Processing {video_name}\")\n",
    "    params = {\"video name\": video_name, \"threshold\": 25, \"min_length\": 5}\n",
    "\n",
    "    video_title = video_name.split(\".\")[0]\n",
    "\n",
    "    experiment_dir = \"results/experiment11\"\n",
    "    experiment_name = f\"experiment11_{video_title}_shift_contours\"\n",
    "\n",
    "    Doc = Documenter(experiment_dir, experiment_name, params, True)\n",
    "    Doc.comment(\"Initial experiment to see whether the splines fit the reaction front.\")\n",
    "\n",
    "    frames = get_video_frames(f\"videos/{params['video name']}\")\n",
    "    # frames are processed inplace by front_from_frames.\n",
    "    frames_raw = get_video_frames(f\"videos/{params['video name']}\")\n",
    "\n",
    "    # Initialize the front\n",
    "    h, w = frames[0].shape\n",
    "    for i, _ in tqdm(enumerate(frames[:-2])):\n",
    "\n",
    "        fig = plt.figure(dpi=400)\n",
    "        plt.imshow(frames_raw[i], cmap=\"gray\")\n",
    "\n",
    "        front = front_from_frames(frames[i], frames[i + 1], threshold=params[\"threshold\"])\n",
    "        front_next = front_from_frames(frames[i + 1], frames[i + 2], threshold=params[\"threshold\"])\n",
    "\n",
    "        contours = contours_from_front(front, min_length=params[\"min_length\"])\n",
    "        contours_next = contours_from_front(front_next, min_length=params[\"min_length\"])\n",
    "\n",
    "        for j, contour in enumerate(contours):\n",
    "\n",
    "            contour = process_contour(contour)\n",
    "\n",
    "            spline, normals = spline_from_contour(contour)\n",
    "\n",
    "            plt.plot(spline[:, 0], spline[:, 1], linewidth=0.25, color=dfki_cmap(250))\n",
    "\n",
    "            for point, normal in zip(spline, normals):\n",
    "                min_dist = dist_to_nearest(point, contours_next)\n",
    "\n",
    "                if min_dist == np.inf:\n",
    "                    continue\n",
    "\n",
    "                plt.quiver(\n",
    "                    point[0],\n",
    "                    point[1],\n",
    "                    normal[0],\n",
    "                    normal[1],\n",
    "                    color=dfki_cmap(dist_to_idx(min_dist)),\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1 / (min_dist + 10e-6),\n",
    "                    width=0.0005,\n",
    "                )\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(experiment_dir + \"/\" + experiment_name + f\"/frame_{i}.png\")\n",
    "        plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T07:33:50.137120Z",
     "start_time": "2024-06-16T07:31:54.413998Z"
    }
   },
   "id": "6decb072ccc6aa43",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
